
@article{redmon_you_2016,
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
	journal = {arXiv:1506.02640 [cs]},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	year = {2016}
}

@article{redmon_yolo9000:_2016,
	title = {{YOLO9000}: {Better}, {Faster}, {Stronger}},
	abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
	journal = {arXiv:1612.08242 [cs]},
	author = {Redmon, Joseph and Farhadi, Ali},
	year = {2016}
}

@article{vadivambal_applications_2011,
	title = {Applications of {Thermal} {Imaging} in {Agriculture} and {Food} {Industry}—{A} {Review}},
	volume = {4},
	abstract = {Thermal imaging is a technique to convert the invisible radiation pattern of an object into visible images for feature extraction and analysis. Infrared thermal imaging was first developed for military purposes but later gained a wide application in various fields such as aerospace, agriculture, civil engineering, medicine, and veterinary. Infrared thermal imaging technology can be applied in all fields where temperature differences could be used to assist in evaluation, diagnosis, or analysis of a process or product. Potential use of thermal imaging in agriculture and food industry includes predicting water stress in crops, planning irrigation scheduling, disease and pathogen detection in plants, predicting fruit yield, evaluating the maturing of fruits, bruise detection in fruits and vegetables, detection of foreign bodies in food material, and temperature distribution during cooking. This paper reviews the application of thermal imaging in agriculture and food industry and elaborates on the potential of thermal imaging in various agricultural practices. The major advantage of infrared thermal imaging is the non-invasive, non-contact, and non-destructive nature of the technique to determine the temperature distribution of any object or process of interest in a short period of time.},
	number = {2},
	journal = {Food and Bioprocess Technology},
	author = {Vadivambal, R. and Jayas, Digvir S.},
	year = {2011},
	pages = {186--199}
}

@article{wagner_multispectral_2016,
	title = {Multispectral {Pedestrian} {Detection} using {Deep} {Fusion} {Convolutional} {Neural} {Networks}},
	abstract = {Robust vision-based pedestrian detection is a crucial feature of future autonomous systems. Thermal cameras provide an additional input channel that helps solving this task and deep convolutional networks are the currently leading approach for many pattern recognition problems, including object detection. In this paper, we explore the potential of deep models for multispectral pedestrian detection. We investigate two deep fusion architectures and analyze their performance on multispectral data. Our results show that a pre-trained late-fusion architecture signiﬁcantly outperforms the current state-of-the-art ACF+T+THOG solution.},
	language = {en},
	journal = {Computational Intelligence},
	author = {Wagner, Jorg and Fischer, Volker and Herman, Michael and Behnke, Sven},
	year = {2016},
	pages = {6}
}

@article{qiao_pork_2007,
	series = {Future of {Food} {Engineering}},
	title = {Pork quality and marbling level assessment using a hyperspectral imaging system},
	volume = {83},
	abstract = {Pork quality is usually evaluated subjectively based on color, texture and exudation characteristics of the meat. In this study, a hyperspectral imaging-based technique was evaluated for rapid, accurate and objective assessment of pork quality. In addition, marbling level was also automatically determined. The system was able to extract spectral characteristics of pork samples. Appropriate spatial features were obtained for marbling distribution in pork meat. Existing marbling standards were scanned, and indices of the marbling scores were formulated by co-occurrence matrix. The principal component analysis (PCA) method was used to compress the entire spectral wavelengths (430–1000nm) into 5, 10 and 20 principal components (PCs), which were then clustered into quality groups. Artificial neural network was used to classify these groups. Results showed that reddish, firm and non-exudative (RFN) and reddish, soft and exudative (RSE) samples were successfully grouped; the total corrected ratio was 75–80\%. The feed-forward neural network model yielded corrected classification as 69\% by 5 PCs and 85\% by 10 PCs. Angular second moment was successfully used to determine marbling scores excepting the score at 10.0. Forty samples were sorted and the result showed that the samples’ marbling score ranged from 3.0 to 5.0.},
	number = {1},
	journal = {Journal of Food Engineering},
	author = {Qiao, Jun and Ngadi, Michael O. and Wang, Ning and Gariépy, Claude and Prasher, Shiv. O.},
	year = {2007},
	pages = {10--16}
}

@article{liu_categorization_2010,
	title = {Categorization of pork quality using {Gabor} filter-based hyperspectral imaging technology},
	volume = {99},
	abstract = {Objective assessment of pork quality is important for meat industry application. Previous studies using spectral approaches focused on using color and exudation features for the determination of pork quality levels without considering the image texture feature. In this study, a Gabor filter-based hyperspectral imaging technique was presented to develop an accurate system for pork quality level classification. Texture features were obtained by filtering hyperspectral images with two-dimensional Gabor functions. Different spectral features were extracted from Gabor-filtered images and hyperspectral images. The principal component analysis (PCA) was used to compress spectral features over the entire wavelengths (400–1000nm) into principal components (PCs). ‘Hybrid’ PCs were created by combining PCs from hyperspectral images with PC(s) from Gabor-filtered images. Both K-means clustering and linear discriminant analysis (LDA) were applied to classify pork samples. Results showed that, the accuracy of K-mean clustering analysis reached 78\% by 5 hybrid PCs and 83\% by 10 hybrid PCs, which were 15\% and 28\% higher than the results without using texture features. The highest classification accuracy using LDA reached 100\% by 5 hybrid PCs. Furthermore, the cross-validation technique was applied for evaluating how the classification results would generalize to independent pork sample sets. A total of 210 partitions (different training and testing sets) were used to obtain the unbiased statistical classification results. The overall classification accuracy reached 84±1\% (mean±95\% confidence interval) by 5 hybrid PCs and was 72±2\% when no Gabor filter-based spectral features were used. Thus, a statistically significant improvement was achieved using image texture features. Moreover, the classification results strongly suggested that the texture features along the direction of π/4 offered more useful information for the differentiation of the four main levels of pork quality.},
	number = {3},
	journal = {Journal of Food Engineering},
	author = {Liu, L. and Ngadi, M. O. and Prasher, S. O. and Gariépy, C.},
	year = {2010},
	pages = {284--293}
}

@article{zhao_spectralspatial_2016,
	title = {Spectral–{Spatial} {Feature} {Extraction} for {Hyperspectral} {Image} {Classification}: {A} {Dimension} {Reduction} and {Deep} {Learning} {Approach}},
	volume = {54},
	shorttitle = {Spectral–{Spatial} {Feature} {Extraction} for {Hyperspectral} {Image} {Classification}},
	abstract = {In this paper, we propose a spectral-spatial feature based classification (SSFC) framework that jointly uses dimension reduction and deep learning techniques for spectral and spatial feature extraction, respectively. In this framework, a balanced local discriminant embedding algorithm is proposed for spectral feature extraction from high-dimensional hyperspectral data sets. In the meantime, convolutional neural network is utilized to automatically find spatial-related features at high levels. Then, the fusion feature is extracted by stacking spectral and spatial features together. Finally, the multiple-feature-based classifier is trained for image classification. Experimental results on well-known hyperspectral data sets show that the proposed SSFC method outperforms other commonly used methods for hyperspectral image classification.},
	number = {8},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Zhao, W. and Du, S.},
	year = {2016},
	pages = {4544--4554}
}

@article{guo_face_2017,
	title = {Face recognition using both visible light image and near-infrared image and a deep network},
	volume = {2},
	abstract = {In recent years, deep networks has achieved outstanding performance in computer vision, especially in the field of face recognition. In terms of the performance for a face recognition model based on deep network, there are two main closely related factors: 1) the structure of the deep neural network, and 2) the number and quality of training data. In real applications, illumination change is one of the most important factors that significantly affect the performance of face recognition algorithms. As for deep network models, only if there is sufficient training data that has various illumination intensity could they achieve expected performance. However, such kind of training data is hard to collect in the real world. In this paper, focusing on the illumination change challenge, we propose a deep network model which takes both visible light image and near-infrared image into account to perform face recognition. Near-infrared image, as we know, is much less sensitive to illuminations. Visible light face image contains abundant texture information which is very useful for face recognition. Thus, we design an adaptive score fusion strategy which hardly has information loss and the nearest neighbor algorithm to conduct the final classification. The experimental results demonstrate that the model is very effective in real-world scenarios and perform much better in terms of illumination change than other state-of-the-art models. The code and resources of this paper are available at http://www.yongxu.org/lunwen.html.},
	number = {1},
	journal = {CAAI Transactions on Intelligence Technology},
	author = {Guo, Kai and Wu, Shuai and Xu, Yong},
	year = {2017},
	pages = {39--47}
}

@inproceedings{li_infrared_2018,
	title = {Infrared and {Visible} {Image} {Fusion} using a {Deep} {Learning} {Framework}},
	abstract = {In recent years, deep learning has become a very active research tool which is used in many image processing fields. In this paper, we propose an effective image fusion method using a deep learning framework to generate a single image which contains all the features from infrared and visible images. First, the source images are decomposed into base parts and detail content. Then the base parts are fused by weighted-averaging. For the detail content, we use a deep learning network to extract multi-layer features. Using these features, we use l1-norm and weighted-average strategy to generate several candidates of the fused detail content. Once we get these candidates, the max selection strategy is used to get the final fused detail content. Finally, the fused image will be reconstructed by combining the fused base part and the detail content. The experimental results demonstrate that our proposed method achieves state-of-the-art performance in both objective assessment and visual quality. The Code of our fusion method is available at https://github.com/exceptionLi/imagefusion\_deeplearning.},
	booktitle = {2018 24th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Li, H. and Wu, X. and Kittler, J.},
	year = {2018},
	pages = {2705--2710}
}

@article{gowen_hyperspectral_2007,
	title = {Hyperspectral imaging – an emerging process analytical tool for food quality and safety control},
	volume = {18},
	abstract = {Hyperspectral imaging (HSI) is an emerging platform technology that integrates conventional imaging and spectroscopy to attain both spatial and spectral information from an object. Although HSI was originally developed for remote sensing, it has recently emerged as a powerful process analytical tool for non-destructive food analysis. This paper provides an introduction to hyperspectral imaging: HSI equipment, image acquisition and processing are described; current limitations and likely future applications are discussed. In addition, recent advances in the application of HSI to food safety and quality assessment are reviewed, such as contaminant detection, defect identification, constituent analysis and quality evaluation.},
	number = {12},
	journal = {Trends in Food Science \& Technology},
	author = {Gowen, A. A. and O'Donnell, C. P. and Cullen, P. J. and Downey, G. and Frias, J. M.},
	year = {2007},
	pages = {590--598}
}

@article{chen_deep_2014,
	title = {Deep {Learning}-{Based} {Classification} of {Hyperspectral} {Data}},
	volume = {7},
	abstract = {Classification is one of the most popular topics in hyperspectral remote sensing. In the last two decades, a huge number of methods were proposed to deal with the hyperspectral data classification problem. However, most of them do not hierarchically extract deep features. In this paper, the concept of deep learning is introduced into hyperspectral data classification for the first time. First, we verify the eligibility of stacked autoencoders by following classical spectral information-based classification. Second, a new way of classifying with spatial-dominated information is proposed. We then propose a novel deep learning framework to merge the two features, from which we can get the highest classification accuracy. The framework is a hybrid of principle component analysis (PCA), deep learning architecture, and logistic regression. Specifically, as a deep learning architecture, stacked autoencoders are aimed to get useful high-level features. Experimental results with widely-used hyperspectral data indicate that classifiers built in this deep learning-based framework provide competitive performance. In addition, the proposed joint spectral-spatial deep neural network opens a new window for future research, showcasing the deep learning-based methods' huge potential for accurate hyperspectral data classification.},
	number = {6},
	journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {Chen, Y. and Lin, Z. and Zhao, X. and Wang, G. and Gu, Y.},
	year = {2014},
	pages = {2094--2107}
}

@misc{hu_deep_2015,
	type = {Research article},
	title = {Deep {Convolutional} {Neural} {Networks} for {Hyperspectral} {Image} {Classification}},
	abstract = {Recently, convolutional neural networks have demonstrated excellent performance on various visual tasks, including the classification of common two-dimensional images. In this paper, deep convolutional neural networks are employed to classify hyperspectral images directly in spectral domain. More specifically, the architecture of the proposed classifier contains five layers with weights which are the input layer, the convolutional layer, the max pooling layer, the full connection layer, and the output layer. These five layers are implemented on each spectral signature to discriminate against others. Experimental results based on several hyperspectral image data sets demonstrate that the proposed method can achieve better classification performance than some traditional methods, such as support vector machines and the conventional deep learning-based methods.},
	language = {en},
	journal = {Journal of Sensors},
	author = {Hu, Wei and Huang, Yangyu and Wei, Li and Zhang, Fan and Li, Hengchao},
	year = {2015}
}

@book{geron_hands-machine_nodate,
	title = {Hands-{On} {Machine} {Learning} with {Scikit}-{Learn} and {TensorFlow}},
	abstract = {dawsonera.com: Direct access to your library's ebook collection},
	language = {en},
	publisher = {O'Reilly Media},
	author = {Géron, Aurélien}
}

@misc{noauthor_zotero_nodate,
	title = {Zotero {\textbar} {People} {\textgreater} lindronics {\textgreater} {Library}},
}

@article{gowen_applications_2010,
	title = {Applications of thermal imaging in food quality and safety assessment},
	volume = {21},
	abstract = {Thermal imaging (TI) is an emerging, non-invasive process analytical technique suitable for the food industry. While TI was originally developed for military applications, it has recently emerged as a powerful non-destructive measurement technique in other industries. This paper provides an overview of TI theory, equipment, and image processing. Recent advances and potential applications of TI for food safety and quality assessment such as temperature validation, bruise and foreign body detection and grain quality evaluation are reviewed.},
	language = {en},
	number = {4},
	journal = {Trends in Food Science \& Technology},
	author = {Gowen, A. A. and Tiwari, B. K. and Cullen, P. J. and McDonnell, K. and O'Donnell, C. P.},
	year = {2010},
	pages = {190--200}
}

@article{ibarra_combined_2000,
	title = {Combined {IR} imaging-neural network method for the estimation of internal temperature in cooked chicken meat},
	volume = {39},
	number = {11},
	journal = {Optical Engineering},
	author = {Ibarra, Juan and Tao, Yang and Xin, Hongwei},
	year = {2000},
	pages = {3032--3038}
}

@inproceedings{lopez_detecting_2017,
	address = {Montreal, QC},
	title = {Detecting exercise-induced fatigue using thermal imaging and deep learning},
	abstract = {Fatigue has adverse eﬀects in both physical and cognitive abilities. Hence, automatically detecting exercise-induced fatigue is of importance, especially in order to assist in the planning of eﬀort and resting during exercise sessions. Thermal imaging and facial analysis provide a mean to detect changes in the human body unobtrusively and in variant conditions of pose and illumination. In this context, this paper proposes the automatic detection of exercise-induced fatigue using thermal cameras and facial images, analyzing them using deep convolutional neural networks. Our results indicate that classiﬁcation of fatigued individuals is possible, obtaining an accuracy that reaches over 80\% when utilizing single thermal images.},
	language = {en},
	booktitle = {2017 {Seventh} {International} {Conference} on {Image} {Processing} {Theory}, {Tools} and {Applications} ({IPTA})},
	publisher = {IEEE},
	author = {Lopez, Miguel Bordallo and del-Blanco, Carlos R. and Garcia, Narciso},
	year = {2017},
	pages = {1--6}
}

@article{mocanu_deep_2016,
	title = {Deep learning for estimating building energy consumption},
	volume = {6},
	abstract = {To improve the design of the electricity infrastructure and the efficient deployment of distributed and renewable energy sources, a new paradigm for the energy supply chain is emerging, leading to the development of smart grids. There is a need to add intelligence at all levels in the grid, acting over various time horizons. Predicting the behavior of the energy system is crucial to mitigate potential uncertainties. An accurate energy prediction at the customer level will reflect directly in efficiency improvements in the whole system. However, prediction of building energy consumption is complex due to many influencing factors, such as climate, performance of thermal systems, and occupancy patterns. Therefore, current state-of-the-art methods are not able to confine the uncertainty at the building level due to the many fluctuations in influencing variables. As an evolution of artificial neural network (ANN)-based prediction methods, deep learning techniques are expected to increase the prediction accuracy by allowing higher levels of abstraction. In this paper, we investigate two newly developed stochastic models for time series prediction of energy consumption, namely Conditional Restricted Boltzmann Machine (CRBM) and Factored Conditional Restricted Boltzmann Machine (FCRBM). The assessment is made on a benchmark dataset consisting of almost four years of one minute resolution electric power consumption data collected from an individual residential customer. The results show that for the energy prediction problem solved here, FCRBM outperforms ANN, Support Vector Machine (SVM), Recurrent Neural Networks (RNN) and CRBM.},
	language = {en},
	journal = {Sustainable Energy, Grids and Networks},
	author = {Mocanu, Elena and Nguyen, Phuong H. and Gibescu, Madeleine and Kling, Wil L.},
	year = {2016},
	pages = {91--99}
}

@inproceedings{farhan_predicting_2015,
	title = {Predicting individual thermal comfort using machine learning algorithms},
	abstract = {Human thermal sensation in an environment may be delayed, which may lead to life threatening conditions, such as hypothermia and hyperthermia. This is especially true for senior citizens, as aging alters the thermal perception in humans. We envision a decision support system that predicts human thermal comfort in real-time using various environmental conditions as well psychological and physiological features, and suggest corresponding actions, which can significantly improve overall thermal comfort and health of individuals, especially senior citizens. The key to realize this vision is an accurate thermal comfort model. We propose a novel machine learning based approach to learn an individual's thermal comfort model. This approach identifies the best set of features, and then learns a classifier that takes a feature vector as input and outputs a corresponding thermal sensation class (i.e. “feeling cold”, “neutral” and “feeling warm”). Evaluation using a large-scale publicly available data demonstrates that when using Support Vector Machines (SVM) classifiers, the accuracy of our approach is 76.7\%, over two times higher than that of the widely adopted Fanger's model (which only achieves accuracy of 35.4\%). In addition, our study indicates that two factors, a person's age and outdoor temperature that are not included in Fanger's model, play an important role in thermal comfort, which is a finding interesting in its own right.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Automation} {Science} and {Engineering} ({CASE})},
	author = {Farhan, Asma Ahmad and Pattipati, Krishna and Wang, Bing and Luh, Peter},
	year = {2015},
	pages = {708--713}
}

@article{cho_deep_2018,
	title = {Deep {Thermal} {Imaging}: {Proximate} {Material} {Type} {Recognition} in the {Wild} through {Deep} {Learning} of {Spatial} {Surface} {Temperature} {Patterns}},
	shorttitle = {Deep {Thermal} {Imaging}},
	abstract = {We introduce Deep Thermal Imaging, a new approach for close-range automatic recognition of materials to enhance the understanding of people and ubiquitous technologies of their proximal environment. Our approach uses a low-cost mobile thermal camera integrated into a smartphone to capture thermal textures. A deep neural network classifies these textures into material types. This approach works effectively without the need for ambient light sources or direct contact with materials. Furthermore, the use of a deep learning network removes the need to handcraft the set of features for different materials. We evaluated the performance of the system by training it to recognise 32 material types in both indoor and outdoor environments. Our approach produced recognition accuracies above 98\% in 14,860 images of 15 indoor materials and above 89\% in 26,584 images of 17 outdoor materials. We conclude by discussing its potentials for real-time use in HCI applications and future directions.},
	journal = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems  - CHI '18},
	author = {Cho, Youngjun and Bianchi-Berthouze, Nadia and Marquardt, Nicolai and Julier, Simon J.},
	year = {2018},
	pages = {1--13}
}

@article{nowruzi_-vehicle_nodate,
	title = {In-{Vehicle} {Occupancy} {Detection} {With} {Convolutional} {Networks} on {Thermal} {Images}},
	abstract = {Counting people is a growing ﬁeld of interest for researchers in recent years. In-vehicle passenger counting is an interesting problem in this domain that has several applications including High Occupancy Vehicle (HOV) lanes. In this paper, present a new in-vehicle thermal image dataset. We propose a tiny convolutional model to count on-board passengers and compare it to well known methods. We show that our model surpasses state-of-the-art methods in classiﬁcation and has comparable performance in detection. Moreover, our model outperforms the state-of-the-art architectures in terms of speed, making it suitable for deployment on embedded platforms. We present the results of multiple deep learning models and thoroughly analyze them.},
	language = {en},
	author = {Nowruzi, Farzan Erlik and Ahmar, Wassim A El and Laganiere, Robert and Ghods, Amir H},
	pages = {8}
}

@article{baranowski_detection_2012,
	title = {Detection of early bruises in apples using hyperspectral data and thermal imaging},
	volume = {110},
	abstract = {The early detection of bruises in apples was studied using a system that included hyperspectral cameras equipped with sensors working in the visible and near-infrared (400–1000nm), short wavelength infrared (1000–2500nm) and thermal imaging camera in mid-wavelength infrared (3500–5000nm) ranges. The principal components analysis (PCA) and minimum noise fraction (MNF) analyses of the images that were captured in particular ranges made it possible to distinguish between areas with defects in the tissue and the sound ones. The fast Fourier analysis of the image sequences after pulse heating of the fruit surface provided additional information not only about the position of the area of damaged tissue but also about its depth. The comparison of the results obtained with supervised classification methods, including soft independent modelling of class analogy (SIMCA), linear discriminant analysis (LDA) and support vector machines (SVM) confirmed that broad spectrum range (400–5000nm) of fruit surface imaging can improve the detection of early bruises with varying depths.},
	language = {en},
	number = {3},
	journal = {Journal of Food Engineering},
	author = {Baranowski, Piotr and Mazurek, Wojciech and Wozniak, Joanna and Majewska, Urszula},
	year = {2012},
	pages = {345--355}
}

@inproceedings{abdul_multi-disnet_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Multi-{DisNet}: {Machine} {Learning}-{Based} {Object} {Distance} {Estimation} from {Multiple} {Cameras}},
	shorttitle = {Multi-{DisNet}},
	abstract = {In this paper, a novel method for distance estimation from multiple cameras to the object viewed with these cameras is presented. The core element of the method is multilayer neural network named Multi-DisNet, which is used to learn the relationship between the sizes of the object bounding boxes in the cameras images and the distance between the object and the cameras. The Multi-DisNet was trained using a supervised learning technique where the input features were manually calculated parameters of the objects bounding boxes in the cameras images and outputs were ground-truth distances between the objects and the cameras. The presented distance estimation system can be of benefit for all applications where object (obstacle) distance estimation is essential for the safety such as autonomous driving applications in automotive or railway. The presented object distance estimation system was evaluated on the images of real-world railway scenes. As a proof-of-concept, the results on the fusion of two sensors, an RGB and thermal camera mounted on a moving train, in the Multi-DisNet distance estimation system are shown. Shown results demonstrate both the good performance of Multi-DisNet system to estimate the mid (up to 200 m) and long-range (up to 1000 m) object distance and benefit of sensor fusion to overcome the problem of not reliable object detection.},
	language = {en},
	booktitle = {Computer {Vision} {Systems}},
	publisher = {Springer International Publishing},
	author = {Abdul, Haseeb Muhammad and Danijela, Ristić-Durrant and Axel, Gräser and Milan, Banić and Dušan, Stamenković},
	editor = {Tzovaras, Dimitrios and Giakoumis, Dimitrios and Vincze, Markus and Argyros, Antonis},
	year = {2019},
	pages = {457--469}
}

@article{chen_normalized_2018,
	title = {Normalized {Total} {Gradient}: {A} {New} {Measure} for {Multispectral} {Image} {Registration}},
	volume = {27},
	shorttitle = {Normalized {Total} {Gradient}},
	abstract = {Image registration is a fundamental issue in multispectral image processing. In filter wheel based multispectral imaging systems, the non-coplanar placement of the filters always causes the misalignment of multiple channel images. The selective characteristic of spectral response in multispectral imaging raises two challenges to image registration. First, the intensity levels of a local region may be different in individual channel images. Second, the local intensity may vary rapidly in some channel images while keeps stationary in others. Conventional multimodal measures, such as mutual information, correlation coefficient, and correlation ratio, can register images with different regional intensity levels, but will fail in the circumstance of severe local intensity variation. In this paper, a new measure, namely normalized total gradient (NTG), is proposed for multispectral image registration. The NTG is applied on the difference between two channel images. This measure is based on the key assumption (observation) that the gradient of difference image between two aligned channel images is sparser than that between two misaligned ones. A registration framework, which incorporates image pyramid and global/local optimization, is further introduced for rigid transform. Experimental results validate that the proposed method is effective for multispectral image registration and performs better than conventional methods.},
	number = {3},
	journal = {IEEE Transactions on Image Processing},
	author = {Chen, Shu-Jie and Shen, Hui-Liang},
	year = {2018},
	pages = {1297--1310}
}

@inproceedings{istenic_thermal_2007,
	title = {Thermal and {Visual} {Image} {Registration} in {Hough} {Parameter} {Space}},
	abstract = {This paper introduces a new approach in image registration, that is a multisensor registration in Hough parameter space. Visual and thermal images of building fronts were aimed to be aligned in order to inspect thermal properties of buildings. Some preprocessing of visible images was necessary to be comparable to their thermal counterparts, namely downsampling and color space conversion from RGB to grayscale intensity. For each image pair, edges were detected with Canny edge detector and, as a result, binary edge images were obtained. These images were further processed by Hough transform which extracted all linear image segments. We decided for linear segments, because they are the most frequent feature appearing in the images of buildings. In the Hough parameter space the rotation and translation of the linear segments can be recovered using the line correspondence analysis. The method was verified first on synthetic images with only translation, only rotation, and also both the rotation and translation together. Finally, a verification on real images was done. The method was able to correctly register both type of images, synthetic and the real ones. In general, our algorithm can cope with rotated and translated images if only a few linear segments are detectible.},
	author = {Istenic, R. and Heric, D. and Ribaric, S. and Zazula, Damjan},
	year = {2007},
	pages = {106--109}
}

@article{myronenko_intensity-based_2010,
	title = {Intensity-{Based} {Image} {Registration} by {Minimizing} {Residual} {Complexity}},
	volume = {29},
	abstract = {Accurate definition of the similarity measure is a key component in image registration. Most commonly used intensity-based similarity measures rely on the assumptions of independence and stationarity of the intensities from pixel to pixel. Such measures cannot capture the complex interactions among the pixel intensities, and often result in less satisfactory registration performances, especially in the presence of spatially-varying intensity distortions. We propose a novel similarity measure that accounts for intensity nonstationarities and complex spatially-varying intensity distortions in mono-modal settings. We derive the similarity measure by analytically solving for the intensity correction field and its adaptive regularization. The final measure can be interpreted as one that favors a registration with minimum compression complexity of the residual image between the two registered images. One of the key advantages of the new similarity measure is its simplicity in terms of both computational complexity and implementation. This measure produces accurate registration results on both artificial and real-world problems that we have tested, and outperforms other state-of-the-art similarity measures in these cases.},
	number = {11},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Myronenko, Andriy and Song, Xubo},
	year = {2010},
	pages = {1882--1891}
}

@inproceedings{mikolajczyk_data_2018,
	title = {Data augmentation for improving deep learning in image classification problem},
	abstract = {These days deep learning is the fastest-growing field in the field of Machine Learning (ML) and Deep Neural Networks (DNN). Among many of DNN structures, the Convolutional Neural Networks (CNN) are currently the main tool used for the image analysis and classification purposes. Although great achievements and perspectives, deep neural networks and accompanying learning algorithms have some relevant challenges to tackle. In this paper, we have focused on the most frequently mentioned problem in the field of machine learning, that is the lack of sufficient amount of the training data or uneven class balance within the datasets. One of the ways of dealing with this problem is so called data augmentation. In the paper we have compared and analyzed multiple methods of data augmentation in the task of image classification, starting from classical image transformations like rotating, cropping, zooming, histogram based methods and finishing at Style Transfer and Generative Adversarial Networks, along with the representative examples. Next, we presented our own method of data augmentation based on image style transfer. The method allows to generate the new images of high perceptual quality that combine the content of a base image with the appearance of another ones. The newly created images can be used to pre-train the given neural network in order to improve the training process efficiency. Proposed method is validated on the three medical case studies: skin melanomas diagnosis, histopathological images and breast magnetic resonance imaging (MRI) scans analysis, utilizing the image classification in order to provide a diagnose. In such kind of problems the data deficiency is one of the most relevant issues. Finally, we discuss the advantages and disadvantages of the methods being analyzed.},
	booktitle = {2018 {International} {Interdisciplinary} {PhD} {Workshop} ({IIPhDW})},
	author = {Mikołajczyk, Agnieszka and Grochowski, Michał},
	year = {2018},
	pages = {117--122}
}

@book{jarc_graz_2007,
	title = {{Texture} features for affine registration of thermal ({FLIR}) and visible images},
	abstract = {Abstract The aim of our research is to analyse the importance of texture information for affine registration of gray-scale far-infrared (FLIR) images and gray-scale images taken in the visible spectrum. Texture features are extracted by Laws texture coefficients and used for computing registration criterion functions. The proposed feature based approach is compared to the commonly used approach, where a registration criterion function is computed directly from intensity features, i.e. grey values. The experiments on a small database of ten image pairs show that our texture feature based registration method works well for the given image modalities. Furthermore, we can expect more robust and more correct registration when texture based criterion function instead of intensity based one is used. 1 Introduction and},
	author = {Jarc, Andreja and Rogelj, Peter and Kovačič, Stanislav},
	publisher = {Citeseer},
	year = {2007}
}

@inproceedings{laws_rapid_1980,
	title = {Rapid {Texture} {Identification}},
	volume = {0238},
	abstract = {A method is presented for classifying each pixel of a textured image, and thus for segmenting the scene. The \&quot;texture energy\&quot; approach requires only a few convolutions with small (typically 5x5) integer coefficient masks, followed by a moving-window absolute average operation. Normalization by the local mean and standard deviation eliminates the need for histogram equalization. Rotation-invariance can also be achieved by using averages of the texture energy features. The convolution masks are separable, and can be implemented with 1-dimensional (vertical and horizontal) or multipass 3x3 convolutions. Special techniques permit rapid processing on general-purpose digital computers.},
	booktitle = {Image {Processing} for {Missile} {Guidance}},
	publisher = {International Society for Optics and Photonics},
	author = {Laws, Kenneth I.},
	year = {1980},
	pages = {376--381}
}

@article{qin_hyperspectral_2013,
	title = {Hyperspectral and multispectral imaging for evaluating food safety and quality},
	volume = {118},
	abstract = {Spectral imaging technologies have been developed rapidly during the past decade. This paper presents hyperspectral and multispectral imaging technologies in the area of food safety and quality evaluation, with an introduction, demonstration, and summarization of current spectral imaging techniques available to the food industry for practical commercial use. The main topics include methods for acquiring spectral images, components for building spectral imaging systems, methods for calibrating spectral imaging systems, and techniques for analyzing spectral images. The applications for evaluating food and agricultural products are presented to reflect common practices of the spectral imaging techniques. Future development of hyperspectral and multispectral imaging is also discussed.},
	language = {en},
	number = {2},
	journal = {Journal of Food Engineering},
	author = {Qin, Jianwei and Chao, Kuanglin and Kim, Moon S. and Lu, Renfu and Burks, Thomas F.},
	year = {2013},
	pages = {157--171}
}

@article{dale_hyperspectral_2013,
	title = {Hyperspectral {Imaging} {Applications} in {Agriculture} and {Agro}-{Food} {Product} {Quality} and {Safety} {Control}: {A} {Review}},
	volume = {48},
	shorttitle = {Hyperspectral {Imaging} {Applications} in {Agriculture} and {Agro}-{Food} {Product} {Quality} and {Safety} {Control}},
	abstract = {In this review, various applications of near-infrared hyperspectral imaging (NIR-HSI) in agriculture and in the quality control of agro-food products are presented. NIR-HSI is an emerging technique that combines classical NIR spectroscopy and imaging techniques in order to simultaneously obtain spectral and spatial information from a field or a sample. The technique is nondestructive, nonpolluting, fast, and relatively inexpensive per analysis. Currently, its applications in agriculture include vegetation mapping, crop disease, stress and yield detection, component identification in plants, and detection of impurities. There is growing interest in HSI for safety and quality assessments of agro-food products. The applications have been classified from the level of satellite images to the macroscopic or molecular level.},
	number = {2},
	journal = {Applied Spectroscopy Reviews},
	author = {Dale, Laura M. and Thewis, André and Boudry, Christelle and Rotar, Ioan and Dardenne, Pierre and Baeten, Vincent and Pierna, Juan A. Fernández},
	year = {2013},
	pages = {142--159}
}

@article{lu_medical_2014,
	title = {Medical hyperspectral imaging: a review},
	volume = {19},
	shorttitle = {Medical hyperspectral imaging},
	abstract = {Hyperspectral imaging (HSI) is an emerging imaging modality for medical applications, especially in disease diagnosis and image-guided surgery. HSI acquires a three-dimensional dataset called hypercube, with two spatial dimensions and one spectral dimension. Spatially resolved spectral imaging obtained by HSI provides diagnostic information about the tissue physiology, morphology, and composition. This review paper presents an overview of the literature on medical hyperspectral imaging technology and its applications. The aim of the survey is threefold: an introduction for those new to the field, an overview for those working in the field, and a reference for those searching for literature on a specific application.},
	number = {1},
	journal = {Journal of Biomedical Optics},
	author = {Lu, Guolan and Fei, Baowei},
	year = {2014},
	pages = {010901}
}

@book{byrnes_unexploded_2008,
	title = {Unexploded {Ordnance} {Detection} and {Mitigation}},
	abstract = {The chapters in this volume were presented at the July–August 2008 NATO Advanced Study Institute on Unexploded Ordnance Detection and Mitigation. The conference was held at the beautiful Il Ciocco resort near Lucca, in the glorious Tuscany region of northern Italy. For the ninth time we gathered at this idyllic spot to explore and extend the reciprocity between mathematics and engineering. The dynamic interaction between world-renowned scientists from the usually disparate communities of pure mathematicians and applied scientists which occurred at our eight previous ASI’s continued at this meeting. The detection and neutralization of unexploded ordnance (UXO) has been of major concern for very many decades; at least since the First World war. UXO continues to be the subject of intensive research in many ?elds of science, incl- ing mathematics, signal processing (mainly radar and sonar) and chemistry. While today’s headlines emphasize the mayhem resulting from the placement of imp- vised explosive devices (IEDs), humanitarian landmine clearing continues to draw signi?cant global attention as well. In many countries of the world, landmines threaten the population and hinder reconstruction and fast, ef?cient utilization of large areas of the mined land in the aftermath of military con?icts.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Byrnes, James},
	year = {2008}
}

@article{kogure_thermodynamic_2007,
	title = {Thermodynamic equilibrium and black-body radiation},
	journal = {The astrophysics of emission-line stars. Springer. p41},
	author = {Kogure, Tomokazu and Leung, Kam-Ching},
	year = {2007}
}

@book{young_sears_2012,
	edition = {13th},
	title = {Sears and {Zemansky}'s university physics: with modern physics},
	publisher = {Pearson Addison-Wesley, San Francisco},
	author = {Young, H.D. and Freedman, R.A. and Ford, A.L. and Sears, F.W. and Zemansky, M.W.},
	year = {2012}
}

@book{burkov_hundred-page_2019,
	address = {Quebec, Canada},
	title = {The hundred-page machine learning book},
	publisher = {Andriy Burkov},
	author = {Burkov, Andriy},
	year = {2019}
}

@article{dyk_art_2001,
	title = {The {Art} of {Data} {Augmentation}},
	volume = {10},
	abstract = {The term data augmentation refers to methods for constructing iterative optimization or sampling algorithms via the introduction of unobserved data or latent variables. For deterministic algorithms, the method was popularized in the general statistical community by the seminal article by Dempster, Laird, and Rubin on the EM algorithm for maximizing a likelihood function or, more generally, a posterior density. For stochastic algorithms, the method was popularized in the statistical literature by Tanner and Wong's Data Augmentation algorithm for posterior sampling and in the physics literature by Swendsen and Wang's algorithm for sampling from the Ising and Potts models and their generalizations; in the physics literature, the method of data augmentation is referred to as the method of auxiliary variables. Data augmentation schemes were used by Tanner and Wong to make simulation feasible and simple, while auxiliary variables were adopted by Swendsen and Wang to improve the speed of iterative simulation. In general, however, constructing data augmentation schemes that result in both simple and fast algorithms is a matter of art in that successful strategies vary greatly with the (observed-data) models being considered. After an overview of data augmentation/auxiliary variables and some recent developments in methods for constructing such efficient data augmentation schemes, we introduce an effective search strategy that combines the ideas of marginal augmentation and conditional augmentation, together with a deterministic approximation method for selecting good augmentation schemes. We then apply this strategy to three common classes of models (specifically, multivariate t, probit regression, and mixed-effects models) to obtain efficient Markov chain Monte Carlo algorithms for posterior sampling. We provide theoretical and empirical evidence that the resulting algorithms, while requiring similar programming effort, can show dramatic improvement over the Gibbs samplers commonly used for these models in practice. A key feature of all these new algorithms is that they are positive recurrent subchains of nonpositive recurrent Markov chains constructed in larger spaces.},
	number = {1},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Dyk, David A. van and Meng, Xiao-Li},
	year = {2001},
	pages = {1--50}
}

@article{perez_effectiveness_2017,
	title = {The {Effectiveness} of {Data} {Augmentation} in {Image} {Classification} using {Deep} {Learning}},
	abstract = {In this paper, we explore and compare multiple solutions to the problem of data augmentation in image classification. Previous work has demonstrated the effectiveness of data augmentation through simple techniques, such as cropping, rotating, and flipping input images. We artificially constrain our access to data to a small subset of the ImageNet dataset, and compare each data augmentation technique in turn. One of the more successful data augmentations strategies is the traditional transformations mentioned above. We also experiment with GANs to generate images of different styles. Finally, we propose a method to allow a neural net to learn augmentations that best improve the classifier, which we call neural augmentation. We discuss the successes and shortcomings of this method on various datasets.},
	journal = {arXiv:1712.04621 [cs]},
	author = {Perez, Luis and Wang, Jason},
	year = {2017}
}

@inproceedings{fawzi_adaptive_2016,
	title = {Adaptive data augmentation for image classification},
	abstract = {Data augmentation is the process of generating samples by transforming training data, with the target of improving the accuracy and robustness of classifiers. In this paper, we propose a new automatic and adaptive algorithm for choosing the transformations of the samples used in data augmentation. Specifically, for each sample, our main idea is to seek a small transformation that yields maximal classification loss on the transformed sample. We employ a trust-region optimization strategy, which consists of solving a sequence of linear programs. Our data augmentation scheme is then integrated into a Stochastic Gradient Descent algorithm for training deep neural networks. We perform experiments on two datasets, and show that that the proposed scheme outperforms random data augmentation algorithms in terms of accuracy and robustness, while yielding comparable or superior results with respect to existing selective sampling approaches.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Fawzi, Alhussein and Samulowitz, Horst and Turaga, Deepak and Frossard, Pascal},
	year = {2016},
	pages = {3688--3692}
}

@article{ronneberger_u-net_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	journal = {arXiv:1505.04597 [cs]},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	year = {2015}
}

@inproceedings{hwang_multispectral_2015,
	title = {Multispectral {Pedestrian} {Detection}: {Benchmark} {Dataset} and {Baseline}},
	shorttitle = {Multispectral {Pedestrian} {Detection}},
	author = {Hwang, Soonmin and Park, Jaesik and Kim, Namil and Choi, Yukyung and So Kweon, In},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	year = {2015},
	pages = {1037--1045}
}

@inproceedings{deng_imagenet_2009,
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	shorttitle = {{ImageNet}},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
	year = {2009},
	pages = {248--255}
}

@incollection{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	pages = {1097--1105}
}

@inproceedings{he_deep_2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	pages = {770--778},
	booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition}
}

@article{ross_information_2003,
	series = {Audio- and {Video}-based {Biometric} {Person} {Authentication} ({AVBPA} 2001)},
	title = {Information fusion in biometrics},
	volume = {24},
	abstract = {User verification systems that use a single biometric indicator often have to contend with noisy sensor data, restricted degrees of freedom, non-universality of the biometric trait and unacceptable error rates. Attempting to improve the performance of individual matchers in such situations may not prove to be effective because of these inherent problems. Multibiometric systems seek to alleviate some of these drawbacks by providing multiple evidences of the same identity. These systems help achieve an increase in performance that may not be possible using a single biometric indicator. Further, multibiometric systems provide anti-spoofing measures by making it difficult for an intruder to spoof multiple biometric traits simultaneously. However, an effective fusion scheme is necessary to combine the information presented by multiple domain experts. This paper addresses the problem of information fusion in biometric verification systems by combining information at the matching score level. Experimental results on combining three biometric modalities (face, fingerprint and hand geometry) are presented.},
	language = {en},
	number = {13},
	journal = {Pattern Recognition Letters},
	author = {Ross, Arun and Jain, Anil},
	year = {2003},
	pages = {2115--2125}
}

@article{wu_towards_2015,
	title = {Towards {Dropout} {Training} for {Convolutional} {Neural} {Networks}},
	volume = {71},
	abstract = {Recently, dropout has seen increasing use in deep learning. For deep convolutional neural networks, dropout is known to work well in fully-connected layers. However, its effect in convolutional and pooling layers is still not clear. This paper demonstrates that max-pooling dropout is equivalent to randomly picking activation based on a multinomial distribution at training time. In light of this insight, we advocate employing our proposed probabilistic weighted pooling, instead of commonly used max-pooling, to act as model averaging at test time. Empirical evidence validates the superiority of probabilistic weighted pooling. We also empirically show that the effect of convolutional dropout is not trivial, despite the dramatically reduced possibility of over-fitting due to the convolutional architecture. Elaborately designing dropout training simultaneously in max-pooling and fully-connected layers, we achieve state-of-the-art performance on MNIST, and very competitive results on CIFAR-10 and CIFAR-100, relative to other approaches without data augmentation. Finally, we compare max-pooling dropout and stochastic pooling, both of which introduce stochasticity based on multinomial distributions at pooling stage.},
	journal = {Neural Networks},
	author = {Wu, Haibing and Gu, Xiaodong},
	year = {2015},
	pages = {1--10}
}

@inproceedings{kwasniewska_deep_2017,
	title = {Deep features class activation map for thermal face detection and tracking},
	abstract = {Recently, capabilities of many computer vision tasks have significantly improved due to advances in Convolutional Neural Networks. In our research, we demonstrate that it can be also used for face detection from low resolution thermal images, acquired with a portable camera. The physical size of the camera used in our research allows for embedding it in a wearable device or indoor remote monitoring solution for elderly and disabled people. The benefits of the proposed architecture were experimentally verified on the thermal video sequences, acquired in various scenarios to address possible limitations of remote diagnostics: movements of the person performing a diagnose and movements of the examined person. The achieved short processing time (42.05±0.21ms) along with high model accuracy (false positives -0.43\%; true positives for the patient focused on a certain task -89.2\%) clearly indicates that the current state of the art in the area of image classification and face tracking in thermography was significantly outperformed.},
	booktitle = {2017 10th {International} {Conference} on {Human} {System} {Interactions} ({HSI})},
	author = {Kwaśniewska, Alicja and Rumiński, Jacek and Rad, Paul},
	year = {2017},
	pages = {41--47}
}

@incollection{thrun_is_1996,
	title = {Is {Learning} {The} n-th {Thing} {Any} {Easier} {Than} {Learning} {The} {First}?},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 8},
	publisher = {MIT Press},
	author = {Thrun, Sebastian},
	editor = {Touretzky, D. S. and Mozer, M. C. and Hasselmo, M. E.},
	year = {1996},
	pages = {640--646}
}

@inproceedings{chappelow_improving_2008,
	title = {Improving supervised classification accuracy using non-rigid multimodal image registration: detecting prostate cancer},
	volume = {6915},
	shorttitle = {Improving supervised classification accuracy using non-rigid multimodal image registration},
	abstract = {Computer-aided diagnosis (CAD) systems for the detection of cancer in medical images require precise labeling of training data. For magnetic resonance (MR) imaging (MRI) of the prostate, training labels define the spatial extent of prostate cancer (CaP); the most common source for these labels is expert segmentations. When ancillary data such as whole mount histology (WMH) sections, which provide the gold standard for cancer ground truth, are available, the manual labeling of CaP can be improved by referencing WMH. However, manual segmentation is error prone, time consuming and not reproducible. Therefore, we present the use of multimodal image registration to automatically and accurately transcribe CaP from histology onto MRI following alignment of the two modalities, in order to improve the quality of training data and hence classifier performance. We quantitatively demonstrate the superiority of this registration-based methodology by comparing its results to the manual CaP annotation of expert radiologists. Five supervised CAD classifiers were trained using the labels for CaP extent on MRI obtained by the expert and 4 different registration techniques. Two of the registration methods were affi;ne schemes; one based on maximization of mutual information (MI) and the other method that we previously developed, Combined Feature Ensemble Mutual Information (COFEMI), which incorporates high-order statistical features for robust multimodal registration. Two non-rigid schemes were obtained by succeeding the two affine registration methods with an elastic deformation step using thin-plate splines (TPS). In the absence of definitive ground truth for CaP extent on MRI, classifier accuracy was evaluated against 7 ground truth surrogates obtained by different combinations of the expert and registration segmentations. For 26 multimodal MRI-WMH image pairs, all four registration methods produced a higher area under the receiver operating characteristic curve compared to that obtained from expert annotation. These results suggest that in the presence of additional multimodal image information one can obtain more accurate object annotations than achievable via expert delineation despite vast differences between modalities that hinder image registration.},
	booktitle = {Medical {Imaging} 2008: {Computer}-{Aided} {Diagnosis}},
	publisher = {International Society for Optics and Photonics},
	author = {Chappelow, Jonathan and Viswanath, Satish and Monaco, James and Rosen, Mark and Tomaszewski, John and Feldman, Michael and Madabhushi, Anant},
	year = {2008},
	pages = {69150V}
}

@article{alom_history_2018,
	title = {The {History} {Began} from {AlexNet}: {A} {Comprehensive} {Survey} on {Deep} {Learning} {Approaches}},
	shorttitle = {The {History} {Began} from {AlexNet}},
	abstract = {Deep learning has demonstrated tremendous success in variety of application domains in the past few years. This new field of machine learning has been growing rapidly and applied in most of the application domains with some new modalities of applications, which helps to open new opportunity. There are different methods have been proposed on different category of learning approaches, which includes supervised, semi-supervised and un-supervised learning. The experimental results show state-of-the-art performance of deep learning over traditional machine learning approaches in the field of Image Processing, Computer Vision, Speech Recognition, Machine Translation, Art, Medical imaging, Medical information processing, Robotics and control, Bio-informatics, Natural Language Processing (NLP), Cyber security, and many more. This report presents a brief survey on development of DL approaches, including Deep Neural Network (DNN), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) including Long Short Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). In addition, we have included recent development of proposed advanced variant DL techniques based on the mentioned DL approaches. Furthermore, DL approaches have explored and evaluated in different application domains are also included in this survey. We have also comprised recently developed frameworks, SDKs, and benchmark datasets that are used for implementing and evaluating deep learning approaches. There are some surveys have published on Deep Learning in Neural Networks [1, 38] and a survey on RL [234]. However, those papers have not discussed the individual advanced techniques for training large scale deep learning models and the recently developed method of generative models [1].},
	journal = {arXiv:1803.01164 [cs]},
	author = {Alom, Md Zahangir and Taha, Tarek M. and Yakopcic, Christopher and Westberg, Stefan and Sidike, Paheding and Nasrin, Mst Shamima and Van Esesn, Brian C. and Awwal, Abdul A. S. and Asari, Vijayan K.},
	year = {2018}
}

@article{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	journal = {arXiv:1409.1556 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	year = {2015}
}

@inproceedings{szegedy_going_2015,
	title = {Going {Deeper} {With} {Convolutions}},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	year = {2015},
	pages = {1--9}
}

@book{kotikalapudi_keras-vis_2017,
	title = {keras-vis},
	publisher = {GitHub},
	author = {Kotikalapudi, Raghavendra and {contributors}},
	year = {2017}
}

@book{iqbal_harisiqbal88plotneuralnet_2018,
	title = {{HarisIqbal88}/{PlotNeuralNet} v1.0.0},
	publisher = {Zenodo},
	author = {Iqbal, Haris},
	year = {2018}
}

@book{chollet_keras_2015,
	title = {Keras},
	author = {Chollet, François and {others}},
	publisher = {GitHub},
	year = {2015}
}

@inproceedings{abadi_tensorflow_2016,
	title = {{TensorFlow}: {A} {System} for {Large}-{Scale} {Machine} {Learning}},
	shorttitle = {{TensorFlow}},
	language = {en},
	author = {Abadi, Martin and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	year = {2016},
	booktitle={12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)},
	pages = {265--283}
}

@inproceedings{shi_benchmarking_2016,
	title = {Benchmarking {State}-of-the-{Art} {Deep} {Learning} {Software} {Tools}},
	abstract = {Deep learning has been shown as a successful machine learning method for a variety of tasks, and its popularity results in numerous open-source deep learning software tools coming to public. Training a deep network is usually a very time-consuming process. To address the huge computational challenge in deep learning, many tools exploit hardware features such as multi-core CPUs and many-core GPUs to shorten the training and inference time. However, different tools exhibit different features and running performance when they train different types of deep networks on different hardware platforms, making it difficult for end users to select an appropriate pair of software and hardware. In this paper, we present our attempt to benchmark several state-of-the-art GPU-accelerated deep learning software tools, including Caffe, CNTK, TensorFlow, and Torch. We focus on evaluating the running time performance (i.e., speed) of these tools with three popular types of neural networks on two representative CPU platforms and three representative GPU platforms. Our contribution is two-fold. First, for end users of deep learning software tools, our benchmarking results can serve as a reference to selecting appropriate hardware platforms and software tools. Second, for developers of deep learning software tools, our in-depth analysis points out possible future directions to further optimize the running performance.},
	booktitle = {2016 7th {International} {Conference} on {Cloud} {Computing} and {Big} {Data} ({CCBD})},
	author = {Shi, Shaohuai and Wang, Qiang and Xu, Pengfei and Chu, Xiaowen},
	year = {2016},
	pages = {99--104}
}

@article{bradski_opencv_2000,
	title = {The {OpenCV} {Library}},
	journal = {Dr. Dobb's Journal of Software Tools},
	author = {Bradski, G.},
	year = {2000}
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	volume = {12},
	journal = {Journal of Machine Learning Research},
	author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	year = {2011},
	pages = {2825--2830}
}

@article{van_der_walt_scikit-image_2014,
	title = {scikit-image: {Image} processing in {Python}},
	volume = {2},
	journal = {PeerJ},
	author = {Van der Walt, Stefan and Schönberger, Johannes L and Nunez-Iglesias, Juan and Boulogne, François and Warner, Joshua D and Yager, Neil and Gouillart, Emmanuelle and Yu, Tony},
	year = {2014},
	pages = {e453}
}

@article{srivastava_dropout_2014,
	title = {Dropout: a simple way to prevent neural networks from overfitting},
	volume = {15},
	shorttitle = {Dropout},
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	pages = {1929--1958}
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016}
}

@article{cai_effective_2019,
	title = {Effective and {Efficient} {Dropout} for {Deep} {Convolutional} {Neural} {Networks}},
	abstract = {Machine-learning-based data-driven applications have become ubiquitous, e.g., health-care analysis and database system optimization. Big training data and large (deep) models are crucial for good performance. Dropout has been widely used as an efficient regularization technique to prevent large models from overfitting. However, many recent works show that dropout does not bring much performance improvement for deep convolutional neural networks (CNNs), a popular deep learning model for data-driven applications. In this paper, we revisit the problem and investigate its failure. We attribute the failure to the conflict between the conventional dropout and the batch normalization operation after it. We propose to adjust the order of the dropout operations to address the conflict; and further, other structurally more suited dropout variants are also examined and introduced for more efficient and effective regularization for CNNs. These dropout variants can be easily integrated into the building blocks of CNNs implemented by existing deep learning libraries, e.g., Apache Singa, to provide effective regularization for CNNs. Extensive experiments on benchmark datasets CIFAR, SVHN and ImageNet are conducted to compare the existing building blocks and the proposed building blocks with the proposed customizable dropout methods. The results confirm the superiority of our building blocks due to the regularization and implicit model ensemble effect of dropout. In particular, we improve over state-of-the-art CNNs with significantly better performance of 3.17\%, 16.15\%, 1.44\%, 21.68\% error rate on CIFAR-10, CIFAR-100, SVHN and ImageNet respectively.},
	journal = {arXiv:1904.03392 [cs]},
	author = {Cai, Shaofeng and Shu, Yao and Wang, Wei and Zhang, Meihui and Chen, Gang and Ooi, Beng Chin},
	year = {2019}
}

@article{tetko_neural_1995,
	title = {Neural network studies. 1. {Comparison} of overfitting and overtraining},
	volume = {35},
	language = {en},
	number = {5},
	journal = {Journal of Chemical Information and Modeling},
	author = {Tetko, Igor V. and Livingstone, David J. and Luik, Alexander I.},
	year = {1995},
	pages = {826--833}
}

@article{radiuk_impact_2017,
	title = {Impact of {Training} {Set} {Batch} {Size} on the {Performance} of {Convolutional} {Neural} {Networks} for {Diverse} {Datasets}},
	volume = {20},
	abstract = {A problem of improving the performance of convolutional neural networks is considered. A parameter of the training set is investigated. The parameter is the batch size. The goal is to find an impact of training set batch size on the performance. To get consistent results, diverse datasets are used. They areMNIST and CIFAR-10. Simplicity of the MNIST dataset stands against complexity of the CIFAR-10 dataset, although the simpler dataset has 10 classes as well as the more complicated one. To achieve acceptable testing results, various convolutional neural network architectures are selected for the MNIST and CIFAR-10 datasets, with two and five convolutional layers, respectively. The assumption about the dependence of the recognition accuracy on the batch size value is confirmed: the larger the batch size value, the higher the recognition accuracy. Another assumption about the impact of the type of the batch size value on the CNN performance is not confirmed.},
	number = {1},
	journal = {Information Technology and Management Science},
	author = {Radiuk, Pavlo M.},
	year = {2017},
	pages = {20--24}
}

@article{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	journal = {arXiv:1502.03167 [cs]},
	author = {Ioffe, Sergey and Szegedy, Christian},
	year = {2015}
}

@article{devarakonda_adabatch_2018,
	title = {{AdaBatch}: {Adaptive} {Batch} {Sizes} for {Training} {Deep} {Neural} {Networks}},
	shorttitle = {{AdaBatch}},
	abstract = {Training deep neural networks with Stochastic Gradient Descent, or its variants, requires careful choice of both learning rate and batch size. While smaller batch sizes generally converge in fewer training epochs, larger batch sizes offer more parallelism and hence better computational efficiency. We have developed a new training approach that, rather than statically choosing a single batch size for all epochs, adaptively increases the batch size during the training process. Our method delivers the convergence rate of small batch sizes while achieving performance similar to large batch sizes. We analyse our approach using the standard AlexNet, ResNet, and VGG networks operating on the popular CIFAR-10, CIFAR-100, and ImageNet datasets. Our results demonstrate that learning with adaptive batch sizes can improve performance by factors of up to 6.25 on 4 NVIDIA Tesla P100 GPUs while changing accuracy by less than 1\% relative to training with fixed batch sizes.},
	journal = {arXiv:1712.02029 [cs, stat]},
	author = {Devarakonda, Aditya and Naumov, Maxim and Garland, Michael},
	year = {2018}
}

@article{smith_dont_2018,
	title = {Don't {Decay} the {Learning} {Rate}, {Increase} the {Batch} {Size}},
	abstract = {It is common practice to decay the learning rate. Here we show one can usually obtain the same learning curve on both training and test sets by instead increasing the batch size during training. This procedure is successful for stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum, and Adam. It reaches equivalent test accuracies after the same number of training epochs, but with fewer parameter updates, leading to greater parallelism and shorter training times. We can further reduce the number of parameter updates by increasing the learning rate \${\textbackslash}epsilon\$ and scaling the batch size \$B {\textbackslash}propto {\textbackslash}epsilon\$. Finally, one can increase the momentum coefficient \$m\$ and scale \$B {\textbackslash}propto 1/(1-m)\$, although this tends to slightly reduce the test accuracy. Crucially, our techniques allow us to repurpose existing training schedules for large batch training with no hyper-parameter tuning. We train ResNet-50 on ImageNet to \$76.1{\textbackslash}\%\$ validation accuracy in under 30 minutes.},
	journal = {arXiv:1711.00489 [cs, stat]},
	author = {Smith, Samuel L. and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V.},
	year = {2018}
}

@article{csaji_approximation_2001,
	title = {Approximation with artificial neural networks},
	volume = {24},
	number = {48},
	journal = {Faculty of Sciences, Etvs Lornd University, Hungary},
	author = {Csáji, Balázs Csanád},
	year = {2001},
	pages = {7}
}

@article{jia_caffe_2014,
	title = {Caffe: {Convolutional} {Architecture} for {Fast} {Feature} {Embedding}},
	shorttitle = {Caffe},
	abstract = {Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (\${\textbackslash}approx\$ 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.},
	journal = {arXiv:1408.5093 [cs]},
	author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
	year = {2014}
}

@article{dollar_fast_2014,
	title = {Fast {Feature} {Pyramids} for {Object} {Detection}},
	volume = {36},
	abstract = {Multi-resolution image features may be approximated via extrapolation from nearby scales, rather than being computed explicitly. This fundamental insight allows us to design object detection algorithms that are as accurate, and considerably faster, than the state-of-the-art. The computational bottleneck of many modern detectors is the computation of features at every scale of a finely-sampled image pyramid. Our key insight is that one may compute finely sampled feature pyramids at a fraction of the cost, without sacrificing performance: for a broad family of features we find that features computed at octave-spaced scale intervals are sufficient to approximate features on a finely-sampled pyramid. Extrapolation is inexpensive as compared to direct feature computation. As a result, our approximation yields considerable speedups with negligible loss in detection accuracy. We modify three diverse visual recognition systems to use fast feature pyramids and show results on both pedestrian detection (measured on the Caltech, INRIA, TUD-Brussels and ETH data sets) and general object detection (measured on the PASCAL VOC). The approach is general and is widely applicable to vision algorithms requiring fine-grained multi-scale analysis. Our approximation is valid for images with broad spectra (most natural images) and fails for images with narrow band-pass spectra (e.g., periodic textures).},
	number = {8},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Dollár, Piotr and Appel, Ron and Belongie, Serge and Perona, Pietro},
	year = {2014},
	pages = {1532--1545}
}

@inproceedings{dalal_histograms_2005,
	title = {Histograms of oriented gradients for human detection},
	volume = {1},
	abstract = {We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
	booktitle = {2005 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}'05)},
	author = {Dalal, N. and Triggs, B.},
	year = {2005},
	pages = {886--893 vol. 1}
}

@article{guan_fusion_2019,
	title = {Fusion of multispectral data through illumination-aware deep neural networks for pedestrian detection},
	volume = {50},
	abstract = {Multispectral pedestrian detection has received extensive attention in recent years as a promising solution to facilitate robust human target detection for around-the-clock applications (e.g., security surveillance and autonomous driving). In this paper, we demonstrate illumination information encoded in multispectral images can be utilized to boost the performance of pedestrian detection significantly. A novel illumination-aware weighting mechanism is present to depict illumination condition of a scene accurately. Such illumination information is incorporated into two-stream deep convolutional neural networks to learn multispectral human-related features under different illumination conditions (daytime and nighttime). Moreover, we utilized illumination information together with multispectral data to generate more accurate semantic segmentation which is used to supervise the training of pedestrian detector. Putting all of the pieces together, we present an effective framework for multispectral pedestrian detection based on multi-task learning of illumination-aware pedestrian detection and semantic segmentation. Our proposed method is trained end-to-end using a well-designed multi-task loss function and outperforms state-of-the-art approaches on KAIST multispectral pedestrian dataset.},
	language = {en},
	journal = {Information Fusion},
	author = {Guan, Dayan and Cao, Yanpeng and Yang, Jiangxin and Cao, Yanlong and Yang, Michael Ying},
	year = {2019},
	pages = {148--157}
}

@inproceedings{konig_fully_2017,
	title = {Fully {Convolutional} {Region} {Proposal} {Networks} for {Multispectral} {Person} {Detection}},
	abstract = {Multispectral images that combine visual-optical (VIS) and infrared (IR) image information are a promising source of data for automatic person detection. Especially in automotive or surveillance applications, challenging conditions such as insufficient illumination or large distances between camera and object occur regularly and can affect image quality. This leads to weak image contrast or low object resolution. In order to detect persons under such conditions, we apply deep learning for effectively fusing the VIS and IR information in multispectral images. We present a novel multispectral Region Proposal Network (RPN) that is built up on the pre-trained very deep convolutional network VGG-16. The proposals of this network are further evaluated using a Boosted Decision Trees classifier in order to reduce potential false positive detections. With a log-average miss rate of 29:83\% on the reasonable test set of the KAIST Multispectral Pedestrian Detection Benchmark, we improve the current state-of-the-art by about 18\%.},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	author = {König, Daniel and Adam, Michael and Jarvers, Christian and Layher, Georg and Neumann, Heiko and Teutsch, Michael},
	year = {2017},
	pages = {243--250}
}

@inproceedings{zhang_how_2016,
	title = {How {Far} {Are} {We} {From} {Solving} {Pedestrian} {Detection}?},
	author = {Zhang, Shanshan and Benenson, Rodrigo and Omran, Mohamed and Hosang, Jan and Schiele, Bernt},
	year = {2016},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages = {1259--1267}
}

@article{he_identity_2016,
	title = {Identity {Mappings} in {Deep} {Residual} {Networks}},
	abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62\% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers},
	journal = {arXiv:1603.05027 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016}
}

@inproceedings{choi_thermal_2012,
	title = {Thermal to visible face recognition},
	volume = {8371},
	abstract = {In low light conditions, visible light face identification is infeasible due to the lack of illumination. For nighttime surveillance, thermal imaging is commonly used because of the intrinsic emissivity of thermal radiation from the human body. However, matching thermal images of faces acquired at nighttime to the predominantly visible light face imagery in existing government databases and watch lists is a challenging task. The difficulty arises from the significant difference between the face's thermal signature and its visible signature (i.e. the modality gap). To match the thermal face to the visible face acquired by the two different modalities, we applied face recognition algorithms that reduce the modality gap in each step of face identification, from low-level analysis to machine learning techniques. Specifically, partial least squares-discriminant analysis (PLS-DA) based approaches were used to correlate the thermal face signatures to the visible face signatures, yielding a thermal-to-visible face identification rate of 49.9\%. While this work makes progress for thermal-to-visible face recognition, more efforts need to be devoted to solving this difficult task. Successful development of a thermal-to-visible face recognition system would significantly enhance the Nation's nighttime surveillance capabilities.},
	booktitle = {Sensing {Technologies} for {Global} {Health}, {Military} {Medicine}, {Disaster} {Response}, and {Environmental} {Monitoring} {II}; and {Biometric} {Technology} for {Human} {Identification} {IX}},
	publisher = {International Society for Optics and Photonics},
	author = {Choi, Jonghyun and Hu, Shuowen and Young, S. Susan and Davis, Larry S.},
	year = {2012},
	pages = {83711L}
}

@article{toet_fusion_1997,
	title = {Fusion of visible and thermal imagery improves situational awareness},
	volume = {18},
	abstract = {A new color image fusion scheme is applied to visible and thermal images of military relevant scenarios. An observer experiment is performed to test if the increased amount of detail in the fused images can improve the accuracy of observers performing a detection and localization task. The results show that observers can localize a target in a scene (1) with a significantly higher accuracy, and (2) with a greater amount of confidence when they perform with fused images (either gray or color fused), compared with the individual image modalities (visible and thermal).},
	language = {en},
	number = {2},
	journal = {Displays},
	author = {Toet, A and IJspeert, J. K and Waxman, A. M and Aguilar, M},
	year = {1997},
	pages = {85--95}
}

@article{davis_background-subtraction_2007,
	series = {Special issue on {Advances} in {Vision} {Algorithms} and {Systems} beyond the {Visible} {Spectrum}},
	title = {Background-subtraction using contour-based fusion of thermal and visible imagery},
	volume = {106},
	abstract = {We present a new background-subtraction technique fusing contours from thermal and visible imagery for persistent object detection in urban settings. Statistical background-subtraction in the thermal domain is used to identify the initial regions-of-interest. Color and intensity information are used within these areas to obtain the corresponding regions-of-interest in the visible domain. Within each region, input and background gradient information are combined to form a Contour Saliency Map. The binary contour fragments, obtained from corresponding Contour Saliency Maps, are then fused into a single image. An A∗ path-constrained search along watershed boundaries of the regions-of-interest is used to complete and close any broken segments in the fused contour image. Lastly, the contour image is flood-filled to produce silhouettes. Results of our approach are evaluated quantitatively and compared with other low- and high-level fusion techniques using manually segmented data.},
	language = {en},
	number = {2},
	journal = {Computer Vision and Image Understanding},
	author = {Davis, James W. and Sharma, Vinay},
	year = {2007},
	pages = {162--182}
}

@article{cooley_fast_1969,
	title = {The {Fast} {Fourier} {Transform} and {Its} {Applications}},
	volume = {12},
	abstract = {The advent of the fast Fourier transform method has greatly extended our ability to implement Fourier methods on digital computers. A description of the alogorithm and its programming is given here and followed by a theorem relating its operands, the finite sample sequences, to the continuous functions they often are intended to approximate. An analysis of the error due to discrete sampling over finite ranges is given in terms of aliasing. Procedures for computing Fourier integrals, convolutions and lagged products are outlined.},
	number = {1},
	journal = {IEEE Transactions on Education},
	author = {Cooley, James W. and Lewis, Peter A. W. and Welch, Peter D.},
	year = {1969},
	pages = {27--34}
}

@article{prinzinger_body_1991,
	title = {Body temperature in birds},
	volume = {99},
	language = {en},
	number = {4},
	journal = {Comparative Biochemistry and Physiology Part A: Physiology},
	author = {Prinzinger, R and Preßmar, A and Schleucher, E},
	year = {1991},
	pages = {499--506}
}

@article{sarfraz_deep_2017,
	title = {Deep {Perceptual} {Mapping} for {Cross}-{Modal} {Face} {Recognition}},
	volume = {122},
	abstract = {Cross modal face matching between the thermal and visible spectrum is a much desired capability for night-time surveillance and security applications. Due to a very large modality gap, thermal-to-visible face recognition is one of the most challenging face matching problem. In this paper, we present an approach to bridge this modality gap by a significant margin. Our approach captures the highly non-linear relationship between the two modalities by using a deep neural network. Our model attempts to learn a non-linear mapping from the visible to the thermal spectrum while preserving the identity information. We show substantive performance improvement on three difficult thermal–visible face datasets. The presented approach improves the state-of-the-art by more than 10 \% on the UND-X1 dataset and by more than 15–30 \% on the NVESD dataset in terms of Rank-1 identification. Our method bridges the drop in performance due to the modality gap by more than 40 \%.},
	language = {en},
	number = {3},
	journal = {International Journal of Computer Vision},
	author = {Sarfraz, M. Saquib and Stiefelhagen, Rainer},
	year = {2017},
	pages = {426--438}
}

@article{maaten_visualizing_2008,
	title = {Visualizing {Data} using t-{SNE}},
	volume = {9},
	number = {Nov},
	journal = {Journal of Machine Learning Research},
	author = {Maaten, Laurens van der and Hinton, Geoffrey},
	year = {2008},
	pages = {2579--2605}
}

@article{simonyan_deep_2014,
	title = {Deep {Inside} {Convolutional} {Networks}: {Visualising} {Image} {Classification} {Models} and {Saliency} {Maps}},
	shorttitle = {Deep {Inside} {Convolutional} {Networks}},
	abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
	journal = {arXiv:1312.6034 [cs]},
	author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	year = {2014}
}

@article{selvaraju_grad-cam_2020,
	title = {Grad-{CAM}: {Visual} {Explanations} from {Deep} {Networks} via {Gradient}-based {Localization}},
	volume = {128},
	shorttitle = {Grad-{CAM}},
	abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.},
	number = {2},
	journal = {International Journal of Computer Vision},
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	year = {2020},
	pages = {336--359}
}

@article{itti_model_1998,
	title = {A model of saliency-based visual attention for rapid scene analysis},
	volume = {20},
	abstract = {A visual attention system, inspired by the behavior and the neuronal architecture of the early primate visual system, is presented. Multiscale image features are combined into a single topographical saliency map. A dynamical neural network then selects attended locations in order of decreasing saliency. The system breaks down the complex problem of scene understanding by rapidly selecting, in a computationally efficient manner, conspicuous locations to be analyzed in detail.},
	number = {11},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Itti, L. and Koch, C. and Niebur, E.},
	year = {1998},
	pages = {1254--1259}
}

@article{redmon_yolov3_2018,
	title = {{YOLOv3}: {An} {Incremental} {Improvement}},
	shorttitle = {{YOLOv3}},
	abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
	journal = {arXiv:1804.02767 [cs]},
	author = {Redmon, Joseph and Farhadi, Ali},
	year = {2018}
}

@article{lecun_backpropagation_1989,
	title = {Backpropagation {Applied} to {Handwritten} {Zip} {Code} {Recognition}},
	volume = {1},
	abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
	number = {4},
	journal = {Neural Computation},
	author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
	year = {1989},
	pages = {541--551}
}

@article{ramachandran_searching_2017,
	title = {Searching for {Activation} {Functions}},
	abstract = {The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, \$f(x) = x {\textbackslash}cdot {\textbackslash}text\{sigmoid\}({\textbackslash}beta x)\$, which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9{\textbackslash}\% for Mobile NASNet-A and 0.6{\textbackslash}\% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.},
	journal = {arXiv:1710.05941 [cs]},
	author = {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V.},
	year = {2017}
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	copyright = {1986 Nature Publishing Group},
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	language = {en},
	number = {6088},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	year = {1986},
	pages = {533--536}
}

@article{hornik_approximation_1991,
	title = {Approximation capabilities of multilayer feedforward networks},
	volume = {4},
	abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(μ) performance criteria, for arbitrary finite input environment measures μ, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.},
	language = {en},
	number = {2},
	journal = {Neural Networks},
	author = {Hornik, Kurt},
	year = {1991},
	pages = {251--257}
}

@inproceedings{kakkirala_thermal_2017,
	title = {Thermal {Infrared} {Face} {Recognition}: {A} {Review}},
	shorttitle = {Thermal {Infrared} {Face} {Recognition}},
	abstract = {Visible light face recognition has been well researched but still its performance is limited by varying illumination conditions. Illumination conditions are major source of the uncertainty in face recognition systems performance when it is used in outdoor setting. In order to augment the performance of visible light face recognition system's performance, infrared facial images as a new modality has been used in the literature. The other intriguing factor in using the infrared, especially thermal infrared imaging for facial recognition is the night time surveillance with little or no light to illuminate the faces. Thermal facial recognition is used in several covert military applications. The covert data acquisition has its own environmental constraints. The choice of infrared makes the system less dependent on external light sources and more robust with respect to incident angle and light variations. In this paper we focus on reviewing recent developments in thermal IR face recognition and suggest the approaches that could be useful in matching visible and thermal IR images.},
	booktitle = {2017 {UKSim}-{AMSS} 19th {International} {Conference} on {Computer} {Modelling} {Simulation} ({UKSim})},
	author = {Kakkirala, Krishna Rao and Chalamala, Srinivasa Rao and Jami, Santosh Kumar},
	year = {2017},
	pages = {55--60}
}

@article{osin_fast_2018,
	title = {Fast multispectral deep fusion networks},
	volume = {Vol. 66},
	number = {nr 6},
	journal = {Bulletin of the Polish Academy of Sciences. Technical Sciences},
	author = {Osin, V. and Cichocki, A. and Burnaev, E.},
	year = {2018}
}

@inproceedings{liu_ssd_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{SSD}: {Single} {Shot} {MultiBox} {Detector}},
	shorttitle = {{SSD}},
	abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For 300×300300×300300 {\textbackslash}times 300 input, SSD achieves 74.3 \% mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for 512×512512×512512 {\textbackslash}times 512 input, SSD achieves 76.9 \% mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/tree/ssd.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	pages = {21--37}
}

@inproceedings{kristo_overview_2018,
	title = {An overview of thermal face recognition methods},
	abstract = {The popularity of surveillance systems grows as well as a need for better security systems particularly in a bad lighting conditions or at night. The aim of a security system is to collect as many details as possible to enable a better recognition of persons. In this paper, a comparison of representative thermal face recognition methods will be given, emphasizing their strengths and weaknesses. Then, trends in the development of surveillance and security systems will be outlined such as fusion of visible and thermal images and use of convolutional neural networks. Also, existing challenges of thermal facial recognition and its applications in a real world will be pointed out.},
	booktitle = {2018 41st {International} {Convention} on {Information} and {Communication} {Technology}, {Electronics} and {Microelectronics} ({MIPRO})},
	author = {Krišto, M. and Ivasic-Kos, M.},
	year = {2018},
	pages = {1098--1103}
}

@techreport{selinger_appearance-based_2006,
	title = {Appearance-{Based} {Facial} {Recognition} {Using} {Visible} and {Thermal} {Imagery}: {A} {Comparative} {Study}},
	shorttitle = {Appearance-{Based} {Facial} {Recognition} {Using} {Visible} and {Thermal} {Imagery}},
	abstract = {We present a comprehensive performance analysis of multiple appearance-based face recognition methodologies, on visible and thermal infrared imagery. We compare algorithms within and between modalities in terms of recognition performance, false alarm rates and requirements to achieve specified performance levels. The effect of illumination conditions on recognition performance is emphasized, as it underlines the relative advantage of radiometrically calibrated thermal imagery for face recognition.},
	language = {en},
	institution = {EQUINOX CORP NEW YORK NY},
	author = {Selinger, Andrea and Socolinsky, Diego A.},
	year = {2006}
}

@inproceedings{akhloufi_multispectral_2009,
	title = {Multispectral face recognition using non linear dimensionality reduction},
	volume = {7341},
	abstract = {Face recognition in the infrared spectrum has attracted a lot of interest in recent years. Many of the techniques used in infrared are based on their visible counterpart, especially linear techniques like PCA (Principal Component Analysis) and LDA (Linear Discriminant Analysis). In this work, we introduce non linear dimensionality reduction approaches for multispectral face recognition. For this purpose, the following techniques were developed: global non linear techniques (Kernel-PCA, Kernel-LDA) and local non linear techniques (Local Linear Embedding, Locality Preserving Projection). The performances of these techniques were compared to classical linear techniques for face recognition like PCA and LDA. Two multispectral face recognition databases were used in our experiments: Equinox Face Recognition Database and Laval University Database. Equinox database contains images in the Visible, Short, Mid and Long waves infrared spectrums. Laval database contains images in the Visible, Near, Mid and Long waves infrared spectrums with variations in time and metabolic activity of the subjects. The obtained results are interesting and show the increase in recognition performance using local non linear dimensionality reduction techniques for infrared face recognition, particularly in near and short wave infrared spectrums.},
	booktitle = {Visual {Information} {Processing} {XVIII}},
	publisher = {International Society for Optics and Photonics},
	author = {Akhloufi, Moulay A. and Bendada, Abdelhakim and Batsale, Jean-Christophe},
	year = {2009},
	pages = {73410J}
}

@article{wu_intraspectrum_2020,
	title = {Intraspectrum {Discrimination} and {Interspectrum} {Correlation} {Analysis} {Deep} {Network} for {Multispectral} {Face} {Recognition}},
	volume = {50},
	abstract = {Multispectral images contain rich recognition information since the multispectral camera can reveal information that is not visible to the human eye or to the conventional RGB camera. Due to this characteristic of multispectral images, multispectral face recognition has attracted lots of research interest. Although some multispectral face recognition methods have been presented in the last decade, how to fully and effectively explore the intraspectrum discriminant information and the useful interspectrum correlation information in multispectral face images for recognition has not been well studied. To boost the performance of multispectral face recognition, we propose an intraspectrum discrimination and interspectrum correlation analysis deep network (IDICN) approach. Multiple spectra are divided into several spectrum-sets, with each containing a group of spectra within a small spectral range. The IDICN network contains a set of spectrum-set-specific deep convolutional neural networks attempting to extract spectrum-set-specific features, followed by a spectrum pooling layer, whose target is to select a group of spectra with favorable discriminative abilities adaptively. IDICN jointly learns the nonlinear representations of the selected spectra, such that the intraspectrum Fisher loss and the interspectrum discriminant correlation are minimized. Experiments on the well-known Hong Kong Polytechnic University, Carnegie Mellon University, and the University of Western Australia multispectral face datasets demonstrate the superior performance of the proposed approach over several state-of-the-art methods.},
	number = {3},
	journal = {IEEE Transactions on Cybernetics},
	author = {Wu, Fei and Jing, Xiao-Yuan and Dong, Xiwei and Hu, Ruimin and Yue, Dong and Wang, Lina and Ji, Yi-Mu and Wang, Ruchuan and Chen, Guoliang},
	year = {2020},
	pages = {1009--1022}
}

@inproceedings{konig_fully_2017-1,
	title = {Fully {Convolutional} {Region} {Proposal} {Networks} for {Multispectral} {Person} {Detection}},
	abstract = {Multispectral images that combine visual-optical (VIS) and infrared (IR) image information are a promising source of data for automatic person detection. Especially in automotive or surveillance applications, challenging conditions such as insufficient illumination or large distances between camera and object occur regularly and can affect image quality. This leads to weak image contrast or low object resolution. In order to detect persons under such conditions, we apply deep learning for effectively fusing the VIS and IR information in multispectral images. We present a novel multispectral Region Proposal Network (RPN) that is built up on the pre-trained very deep convolutional network VGG-16. The proposals of this network are further evaluated using a Boosted Decision Trees classifier in order to reduce potential false positive detections. With a log-average miss rate of 29:83\% on the reasonable test set of the KAIST Multispectral Pedestrian Detection Benchmark, we improve the current state-of-the-art by about 18\%.},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	author = {König, Daniel and Adam, Michael and Jarvers, Christian and Layher, Georg and Neumann, Heiko and Teutsch, Michael},
	year = {2017},
	pages = {243--250}
}

@article{simonyan_two-stream_2014,
	title = {Two-{Stream} {Convolutional} {Networks} for {Action} {Recognition} in {Videos}},
	abstract = {We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework. Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multi-task learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.},
	journal = {arXiv:1406.2199 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	year = {2014}
}

@inproceedings{scherer_evaluation_2010,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Evaluation of {Pooling} {Operations} in {Convolutional} {Architectures} for {Object} {Recognition}},
	abstract = {A common practice to gain invariant features in object recognition models is to aggregate multiple low-level features over a small neighborhood. However, the differences between those models makes a comparison of the properties of different aggregation functions hard. Our aim is to gain insight into different functions by directly comparing them on a fixed architecture for several common object recognition tasks. Empirical results show that a maximum pooling operation significantly outperforms subsampling operations. Despite their shift-invariant properties, overlapping pooling windows are no significant improvement over non-overlapping pooling windows. By applying this knowledge, we achieve state-of-the-art error rates of 4.57\% on the NORB normalized-uniform dataset and 5.6\% on the NORB jittered-cluttered dataset.},
	language = {en},
	booktitle = {Artificial {Neural} {Networks} – {ICANN} 2010},
	publisher = {Springer},
	author = {Scherer, Dominik and Müller, Andreas and Behnke, Sven},
	editor = {Diamantaras, Konstantinos and Duch, Wlodek and Iliadis, Lazaros S.},
	year = {2010},
	pages = {92--101}
}

@article{kingma_adam_2014,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	year = {2014}
}

@article{masters_revisiting_2018,
	title = {Revisiting {Small} {Batch} {Training} for {Deep} {Neural} {Networks}},
	abstract = {Modern deep neural network training is typically based on mini-batch stochastic gradient optimization. While the use of large mini-batches increases the available computational parallelism, small batch training has been shown to provide improved generalization performance and allows a significantly smaller memory footprint, which might also be exploited to improve machine throughput. In this paper, we review common assumptions on learning rate scaling and training duration, as a basis for an experimental comparison of test performance for different mini-batch sizes. We adopt a learning rate that corresponds to a constant average weight update per gradient calculation (i.e., per unit cost of computation), and point out that this results in a variance of the weight updates that increases linearly with the mini-batch size \$m\$. The collected experimental results for the CIFAR-10, CIFAR-100 and ImageNet datasets show that increasing the mini-batch size progressively reduces the range of learning rates that provide stable convergence and acceptable test performance. On the other hand, small mini-batch sizes provide more up-to-date gradient calculations, which yields more stable and reliable training. The best performance has been consistently obtained for mini-batch sizes between \$m = 2\$ and \$m = 32\$, which contrasts with recent work advocating the use of mini-batch sizes in the thousands.},
	journal = {arXiv:1804.07612 [cs, stat]},
	author = {Masters, Dominic and Luschi, Carlo},
	year = {2018}
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	year = {1998},
	pages = {2278--2324}
}

@article{krizhevsky_learning_2009,
	title = {Learning multiple layers of features from tiny images},
	author = {Krizhevsky, Alex and Hinton, Geoffrey and {others}},
	year = {2009},
	publisher = {Citeseer},
	journal = {{Technical} {Report}, {University} of {Toronto}},
}

@inproceedings{karpathy_large-scale_2014,
	title = {Large-{Scale} {Video} {Classification} with {Convolutional} {Neural} {Networks}},
	abstract = {Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on large-scale video classification using a new dataset of 1 million YouTube videos belonging to 487 classes. We study multiple approaches for extending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggest a multiresolution, foveated architecture as a promising way of speeding up the training. Our best spatio-temporal networks display significant performance improvements compared to strong feature-based baselines (55.3\% to 63.9\%), but only a surprisingly modest improvement compared to single-frame models (59.3\% to 60.9\%). We further study the generalization performance of our best model by retraining the top layers on the UCF-101 Action Recognition dataset and observe significant performance improvements compared to the UCF-101 baseline model (63.3\% up from 43.9\%).},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Karpathy, Andrej and Toderici, George and Shetty, Sanketh and Leung, Thomas and Sukthankar, Rahul and Fei-Fei, Li},
	year = {2014},
	pages = {1725--1732}
}

@article{hochreiter_long_1997,
	title = {Long {Short}-term {Memory}},
	volume = {9},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	journal = {Neural computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	year = {1997},
	pages = {1735--80}
}