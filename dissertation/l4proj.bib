
@article{redmon_you_2016,
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	shorttitle = {You {Only} {Look} {Once}},
	url = {http://arxiv.org/abs/1506.02640},
	abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
	urldate = {2019-10-31},
	journal = {arXiv:1506.02640 [cs]},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	month = may,
	year = {2016},
	note = {arXiv: 1506.02640},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/lindronics/Zotero/storage/8UI38MJV/Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf:application/pdf;arXiv.org Snapshot:/Users/lindronics/Zotero/storage/SUIWXVST/1506.html:text/html}
}

@article{redmon_yolo9000:_2016,
	title = {{YOLO9000}: {Better}, {Faster}, {Stronger}},
	shorttitle = {{YOLO9000}},
	url = {http://arxiv.org/abs/1612.08242},
	abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
	urldate = {2019-10-31},
	journal = {arXiv:1612.08242 [cs]},
	author = {Redmon, Joseph and Farhadi, Ali},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.08242
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/lindronics/Zotero/storage/TTA8HRUS/Redmon and Farhadi - 2016 - YOLO9000 Better, Faster, Stronger.pdf:application/pdf;arXiv.org Snapshot:/Users/lindronics/Zotero/storage/BI7RMTL9/1612.html:text/html}
}

@article{vadivambal_applications_2011,
	title = {Applications of {Thermal} {Imaging} in {Agriculture} and {Food} {Industry}—{A} {Review}},
	volume = {4},
	issn = {1935-5149},
	url = {https://doi.org/10.1007/s11947-010-0333-5},
	doi = {10.1007/s11947-010-0333-5},
	abstract = {Thermal imaging is a technique to convert the invisible radiation pattern of an object into visible images for feature extraction and analysis. Infrared thermal imaging was first developed for military purposes but later gained a wide application in various fields such as aerospace, agriculture, civil engineering, medicine, and veterinary. Infrared thermal imaging technology can be applied in all fields where temperature differences could be used to assist in evaluation, diagnosis, or analysis of a process or product. Potential use of thermal imaging in agriculture and food industry includes predicting water stress in crops, planning irrigation scheduling, disease and pathogen detection in plants, predicting fruit yield, evaluating the maturing of fruits, bruise detection in fruits and vegetables, detection of foreign bodies in food material, and temperature distribution during cooking. This paper reviews the application of thermal imaging in agriculture and food industry and elaborates on the potential of thermal imaging in various agricultural practices. The major advantage of infrared thermal imaging is the non-invasive, non-contact, and non-destructive nature of the technique to determine the temperature distribution of any object or process of interest in a short period of time.},
	language = {en},
	number = {2},
	urldate = {2019-10-17},
	journal = {Food and Bioprocess Technology},
	author = {Vadivambal, R. and Jayas, Digvir S.},
	month = feb,
	year = {2011},
	keywords = {Agriculture, Food, Infrared radiation, Quality, Thermal imaging},
	pages = {186--199},
	file = {Springer Full Text PDF:/Users/lindronics/Zotero/storage/8WX9SA9E/Vadivambal and Jayas - 2011 - Applications of Thermal Imaging in Agriculture and.pdf:application/pdf}
}

@article{wagner_multispectral_2016,
	title = {Multispectral {Pedestrian} {Detection} using {Deep} {Fusion} {Convolutional} {Neural} {Networks}},
	abstract = {Robust vision-based pedestrian detection is a crucial feature of future autonomous systems. Thermal cameras provide an additional input channel that helps solving this task and deep convolutional networks are the currently leading approach for many pattern recognition problems, including object detection. In this paper, we explore the potential of deep models for multispectral pedestrian detection. We investigate two deep fusion architectures and analyze their performance on multispectral data. Our results show that a pre-trained late-fusion architecture signiﬁcantly outperforms the current state-of-the-art ACF+T+THOG solution.},
	language = {en},
	journal = {Computational Intelligence},
	author = {Wagner, Jorg and Fischer, Volker and Herman, Michael and Behnke, Sven},
	year = {2016},
	pages = {6},
	file = {Wagner et al. - 2016 - Multispectral Pedestrian Detection using Deep Fusi.pdf:/Users/lindronics/Zotero/storage/LLVCBI4S/Wagner et al. - 2016 - Multispectral Pedestrian Detection using Deep Fusi.pdf:application/pdf}
}

@article{qiao_pork_2007,
	series = {Future of {Food} {Engineering}},
	title = {Pork quality and marbling level assessment using a hyperspectral imaging system},
	volume = {83},
	issn = {0260-8774},
	url = {http://www.sciencedirect.com/science/article/pii/S0260877407001185},
	doi = {10.1016/j.jfoodeng.2007.02.038},
	abstract = {Pork quality is usually evaluated subjectively based on color, texture and exudation characteristics of the meat. In this study, a hyperspectral imaging-based technique was evaluated for rapid, accurate and objective assessment of pork quality. In addition, marbling level was also automatically determined. The system was able to extract spectral characteristics of pork samples. Appropriate spatial features were obtained for marbling distribution in pork meat. Existing marbling standards were scanned, and indices of the marbling scores were formulated by co-occurrence matrix. The principal component analysis (PCA) method was used to compress the entire spectral wavelengths (430–1000nm) into 5, 10 and 20 principal components (PCs), which were then clustered into quality groups. Artificial neural network was used to classify these groups. Results showed that reddish, firm and non-exudative (RFN) and reddish, soft and exudative (RSE) samples were successfully grouped; the total corrected ratio was 75–80\%. The feed-forward neural network model yielded corrected classification as 69\% by 5 PCs and 85\% by 10 PCs. Angular second moment was successfully used to determine marbling scores excepting the score at 10.0. Forty samples were sorted and the result showed that the samples’ marbling score ranged from 3.0 to 5.0.},
	number = {1},
	urldate = {2019-10-12},
	journal = {Journal of Food Engineering},
	author = {Qiao, Jun and Ngadi, Michael O. and Wang, Ning and Gariépy, Claude and Prasher, Shiv. O.},
	month = nov,
	year = {2007},
	keywords = {Cluster analysis, Hyperspectral imaging, Marbling, Neural network, PCA, Pork quality},
	pages = {10--16},
	file = {ScienceDirect Full Text PDF:/Users/lindronics/Zotero/storage/4YZRMUY6/Qiao et al. - 2007 - Pork quality and marbling level assessment using a.pdf:application/pdf;ScienceDirect Snapshot:/Users/lindronics/Zotero/storage/VRMCJIJF/S0260877407001185.html:text/html}
}

@article{liu_categorization_2010,
	title = {Categorization of pork quality using {Gabor} filter-based hyperspectral imaging technology},
	volume = {99},
	issn = {0260-8774},
	url = {http://www.sciencedirect.com/science/article/pii/S026087741000110X},
	doi = {10.1016/j.jfoodeng.2010.03.001},
	abstract = {Objective assessment of pork quality is important for meat industry application. Previous studies using spectral approaches focused on using color and exudation features for the determination of pork quality levels without considering the image texture feature. In this study, a Gabor filter-based hyperspectral imaging technique was presented to develop an accurate system for pork quality level classification. Texture features were obtained by filtering hyperspectral images with two-dimensional Gabor functions. Different spectral features were extracted from Gabor-filtered images and hyperspectral images. The principal component analysis (PCA) was used to compress spectral features over the entire wavelengths (400–1000nm) into principal components (PCs). ‘Hybrid’ PCs were created by combining PCs from hyperspectral images with PC(s) from Gabor-filtered images. Both K-means clustering and linear discriminant analysis (LDA) were applied to classify pork samples. Results showed that, the accuracy of K-mean clustering analysis reached 78\% by 5 hybrid PCs and 83\% by 10 hybrid PCs, which were 15\% and 28\% higher than the results without using texture features. The highest classification accuracy using LDA reached 100\% by 5 hybrid PCs. Furthermore, the cross-validation technique was applied for evaluating how the classification results would generalize to independent pork sample sets. A total of 210 partitions (different training and testing sets) were used to obtain the unbiased statistical classification results. The overall classification accuracy reached 84±1\% (mean±95\% confidence interval) by 5 hybrid PCs and was 72±2\% when no Gabor filter-based spectral features were used. Thus, a statistically significant improvement was achieved using image texture features. Moreover, the classification results strongly suggested that the texture features along the direction of π/4 offered more useful information for the differentiation of the four main levels of pork quality.},
	number = {3},
	urldate = {2019-10-12},
	journal = {Journal of Food Engineering},
	author = {Liu, L. and Ngadi, M. O. and Prasher, S. O. and Gariépy, C.},
	month = aug,
	year = {2010},
	keywords = {PCA, Pork quality, -means clustering, Gabor filter, Hyperspectral image, LDA, Texture},
	pages = {284--293},
	file = {ScienceDirect Full Text PDF:/Users/lindronics/Zotero/storage/P2BH26XL/Liu et al. - 2010 - Categorization of pork quality using Gabor filter-.pdf:application/pdf;ScienceDirect Snapshot:/Users/lindronics/Zotero/storage/6P93QLHH/S026087741000110X.html:text/html}
}

@article{zhao_spectralspatial_2016,
	title = {Spectral–{Spatial} {Feature} {Extraction} for {Hyperspectral} {Image} {Classification}: {A} {Dimension} {Reduction} and {Deep} {Learning} {Approach}},
	volume = {54},
	shorttitle = {Spectral–{Spatial} {Feature} {Extraction} for {Hyperspectral} {Image} {Classification}},
	doi = {10.1109/TGRS.2016.2543748},
	abstract = {In this paper, we propose a spectral-spatial feature based classification (SSFC) framework that jointly uses dimension reduction and deep learning techniques for spectral and spatial feature extraction, respectively. In this framework, a balanced local discriminant embedding algorithm is proposed for spectral feature extraction from high-dimensional hyperspectral data sets. In the meantime, convolutional neural network is utilized to automatically find spatial-related features at high levels. Then, the fusion feature is extracted by stacking spectral and spatial features together. Finally, the multiple-feature-based classifier is trained for image classification. Experimental results on well-known hyperspectral data sets show that the proposed SSFC method outperforms other commonly used methods for hyperspectral image classification.},
	number = {8},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Zhao, W. and Du, S.},
	month = aug,
	year = {2016},
	keywords = {Hyperspectral imaging, Algorithm design and analysis, Balanced local discriminant embedding (BLDE), balanced local discriminant embedding algorithm, convolutional neural network, convolutional neural network (CNN), deep learning (DL), deep learning approach, deep learning techniques, dimension reduction (DR), dimension reduction approach, feature extraction, Feature extraction, fusion feature, high-dimensional hyperspectral data sets, hyperspectral image classification, hyperspectral imaging, image classification, Machine learning, spectral-spatial feature based classification framework, spectral-spatial feature extraction, SSFC framework, Training},
	pages = {4544--4554},
	file = {IEEE Xplore Abstract Record:/Users/lindronics/Zotero/storage/8ULRNRLR/7450160.html:text/html;IEEE Xplore Full Text PDF:/Users/lindronics/Zotero/storage/44V2LBBV/Zhao and Du - 2016 - Spectral–Spatial Feature Extraction for Hyperspect.pdf:application/pdf}
}

@article{guo_face_2017,
	title = {Face recognition using both visible light image and near-infrared image and a deep network},
	volume = {2},
	issn = {2468-2322},
	url = {http://www.sciencedirect.com/science/article/pii/S2468232217300148},
	doi = {10.1016/j.trit.2017.03.001},
	abstract = {In recent years, deep networks has achieved outstanding performance in computer vision, especially in the field of face recognition. In terms of the performance for a face recognition model based on deep network, there are two main closely related factors: 1) the structure of the deep neural network, and 2) the number and quality of training data. In real applications, illumination change is one of the most important factors that significantly affect the performance of face recognition algorithms. As for deep network models, only if there is sufficient training data that has various illumination intensity could they achieve expected performance. However, such kind of training data is hard to collect in the real world. In this paper, focusing on the illumination change challenge, we propose a deep network model which takes both visible light image and near-infrared image into account to perform face recognition. Near-infrared image, as we know, is much less sensitive to illuminations. Visible light face image contains abundant texture information which is very useful for face recognition. Thus, we design an adaptive score fusion strategy which hardly has information loss and the nearest neighbor algorithm to conduct the final classification. The experimental results demonstrate that the model is very effective in real-world scenarios and perform much better in terms of illumination change than other state-of-the-art models. The code and resources of this paper are available at http://www.yongxu.org/lunwen.html.},
	number = {1},
	urldate = {2019-10-08},
	journal = {CAAI Transactions on Intelligence Technology},
	author = {Guo, Kai and Wu, Shuai and Xu, Yong},
	month = mar,
	year = {2017},
	keywords = {Deep network, Face recognition, Illumination change, Insufficient training data},
	pages = {39--47},
	file = {ScienceDirect Full Text PDF:/Users/lindronics/Zotero/storage/26UFS9LP/Guo et al. - 2017 - Face recognition using both visible light image an.pdf:application/pdf;ScienceDirect Snapshot:/Users/lindronics/Zotero/storage/WGC6NSZB/S2468232217300148.html:text/html}
}

@inproceedings{li_infrared_2018,
	title = {Infrared and {Visible} {Image} {Fusion} using a {Deep} {Learning} {Framework}},
	doi = {10.1109/ICPR.2018.8546006},
	abstract = {In recent years, deep learning has become a very active research tool which is used in many image processing fields. In this paper, we propose an effective image fusion method using a deep learning framework to generate a single image which contains all the features from infrared and visible images. First, the source images are decomposed into base parts and detail content. Then the base parts are fused by weighted-averaging. For the detail content, we use a deep learning network to extract multi-layer features. Using these features, we use l1-norm and weighted-average strategy to generate several candidates of the fused detail content. Once we get these candidates, the max selection strategy is used to get the final fused detail content. Finally, the fused image will be reconstructed by combining the fused base part and the detail content. The experimental results demonstrate that our proposed method achieves state-of-the-art performance in both objective assessment and visual quality. The Code of our fusion method is available at https://github.com/exceptionLi/imagefusion\_deeplearning.},
	booktitle = {2018 24th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Li, H. and Wu, X. and Kittler, J.},
	month = aug,
	year = {2018},
	keywords = {feature extraction, Feature extraction, deep learning framework, deep learning network, image fusion, Image fusion, image fusion method, image reconstruction, Image reconstruction, infrared images, infrared imaging, learning (artificial intelligence), max selection strategy, multilayer feature extraction, Pattern recognition, Task analysis, Tools, weighted-average strategy},
	pages = {2705--2710},
	file = {IEEE Xplore Abstract Record:/Users/lindronics/Zotero/storage/FXFUH9GN/8546006.html:text/html;IEEE Xplore Full Text PDF:/Users/lindronics/Zotero/storage/PFMRRHJQ/Li et al. - 2018 - Infrared and Visible Image Fusion using a Deep Lea.pdf:application/pdf}
}

@article{gowen_hyperspectral_2007,
	title = {Hyperspectral imaging – an emerging process analytical tool for food quality and safety control},
	volume = {18},
	issn = {0924-2244},
	url = {http://www.sciencedirect.com/science/article/pii/S0924224407002026},
	doi = {10.1016/j.tifs.2007.06.001},
	abstract = {Hyperspectral imaging (HSI) is an emerging platform technology that integrates conventional imaging and spectroscopy to attain both spatial and spectral information from an object. Although HSI was originally developed for remote sensing, it has recently emerged as a powerful process analytical tool for non-destructive food analysis. This paper provides an introduction to hyperspectral imaging: HSI equipment, image acquisition and processing are described; current limitations and likely future applications are discussed. In addition, recent advances in the application of HSI to food safety and quality assessment are reviewed, such as contaminant detection, defect identification, constituent analysis and quality evaluation.},
	number = {12},
	urldate = {2019-10-01},
	journal = {Trends in Food Science \& Technology},
	author = {Gowen, A. A. and O'Donnell, C. P. and Cullen, P. J. and Downey, G. and Frias, J. M.},
	month = dec,
	year = {2007},
	pages = {590--598},
	file = {ScienceDirect Full Text PDF:/Users/lindronics/Zotero/storage/AJRL5SJN/Gowen et al. - 2007 - Hyperspectral imaging – an emerging process analyt.pdf:application/pdf;ScienceDirect Snapshot:/Users/lindronics/Zotero/storage/E9KS28W2/S0924224407002026.html:text/html}
}

@article{chen_deep_2014,
	title = {Deep {Learning}-{Based} {Classification} of {Hyperspectral} {Data}},
	volume = {7},
	doi = {10.1109/JSTARS.2014.2329330},
	abstract = {Classification is one of the most popular topics in hyperspectral remote sensing. In the last two decades, a huge number of methods were proposed to deal with the hyperspectral data classification problem. However, most of them do not hierarchically extract deep features. In this paper, the concept of deep learning is introduced into hyperspectral data classification for the first time. First, we verify the eligibility of stacked autoencoders by following classical spectral information-based classification. Second, a new way of classifying with spatial-dominated information is proposed. We then propose a novel deep learning framework to merge the two features, from which we can get the highest classification accuracy. The framework is a hybrid of principle component analysis (PCA), deep learning architecture, and logistic regression. Specifically, as a deep learning architecture, stacked autoencoders are aimed to get useful high-level features. Experimental results with widely-used hyperspectral data indicate that classifiers built in this deep learning-based framework provide competitive performance. In addition, the proposed joint spectral-spatial deep neural network opens a new window for future research, showcasing the deep learning-based methods' huge potential for accurate hyperspectral data classification.},
	number = {6},
	journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {Chen, Y. and Lin, Z. and Zhao, X. and Wang, G. and Gu, Y.},
	month = jun,
	year = {2014},
	keywords = {Hyperspectral imaging, feature extraction, Feature extraction, hyperspectral imaging, image classification, Training, learning (artificial intelligence), Autoencoder (AE), classical spectral information-based classification, deep learning, deep learning architecture, deep learning-based classification, geophysical image processing, hyperspectral data classification, logistic regression, Logistics, neural nets, principal component analysis, Principal component analysis, principle component analysis, regression analysis, remote sensing, spatial-dominated information, spectral-spatial deep neural network, stacked autoencoder (SAE), stacked autoencoders, support vector machine (SVM), Support vector machines},
	pages = {2094--2107},
	file = {IEEE Xplore Abstract Record:/Users/lindronics/Zotero/storage/ETVWMPXS/6844831.html:text/html;IEEE Xplore Full Text PDF:/Users/lindronics/Zotero/storage/47DRLJ62/Chen et al. - 2014 - Deep Learning-Based Classification of Hyperspectra.pdf:application/pdf}
}

@misc{hu_deep_2015,
	type = {Research article},
	title = {Deep {Convolutional} {Neural} {Networks} for {Hyperspectral} {Image} {Classification}},
	url = {https://www.hindawi.com/journals/js/2015/258619/},
	abstract = {Recently, convolutional neural networks have demonstrated excellent performance on various visual tasks, including the classification of common two-dimensional images. In this paper, deep convolutional neural networks are employed to classify hyperspectral images directly in spectral domain. More specifically, the architecture of the proposed classifier contains five layers with weights which are the input layer, the convolutional layer, the max pooling layer, the full connection layer, and the output layer. These five layers are implemented on each spectral signature to discriminate against others. Experimental results based on several hyperspectral image data sets demonstrate that the proposed method can achieve better classification performance than some traditional methods, such as support vector machines and the conventional deep learning-based methods.},
	language = {en},
	urldate = {2019-09-26},
	journal = {Journal of Sensors},
	author = {Hu, Wei and Huang, Yangyu and Wei, Li and Zhang, Fan and Li, Hengchao},
	year = {2015},
	doi = {10.1155/2015/258619},
	file = {Full Text PDF:/Users/lindronics/Zotero/storage/IL6YKX4Q/Hu et al. - 2015 - Deep Convolutional Neural Networks for Hyperspectr.pdf:application/pdf;Snapshot:/Users/lindronics/Zotero/storage/SNGUKZQB/258619.html:application/xhtml+xml}
}

@book{geron_hands-machine_nodate,
	title = {Hands-{On} {Machine} {Learning} with {Scikit}-{Learn} and {TensorFlow}},
	isbn = {978-1-4919-6226-8},
	url = {https://www.dawsonera.com/abstract/9781491962268},
	abstract = {dawsonera.com: Direct access to your library's ebook collection},
	language = {en},
	urldate = {2019-09-25},
	publisher = {O'Reilly Media},
	author = {Géron, Aurélien},
	file = {Snapshot:/Users/lindronics/Zotero/storage/YK6Q4IX6/9781491962268.html:text/html}
}

@misc{noauthor_zotero_nodate,
	title = {Zotero {\textbar} {People} {\textgreater} lindronics {\textgreater} {Library}},
	url = {https://www.zotero.org/lindronics/items},
	urldate = {2019-09-25},
	file = {Zotero | People > lindronics > Library:/Users/lindronics/Zotero/storage/IW8NKBVI/items.html:text/html}
}

@article{gowen_applications_2010,
	title = {Applications of thermal imaging in food quality and safety assessment},
	volume = {21},
	issn = {0924-2244},
	url = {http://www.sciencedirect.com/science/article/pii/S092422440900301X},
	doi = {10.1016/j.tifs.2009.12.002},
	abstract = {Thermal imaging (TI) is an emerging, non-invasive process analytical technique suitable for the food industry. While TI was originally developed for military applications, it has recently emerged as a powerful non-destructive measurement technique in other industries. This paper provides an overview of TI theory, equipment, and image processing. Recent advances and potential applications of TI for food safety and quality assessment such as temperature validation, bruise and foreign body detection and grain quality evaluation are reviewed.},
	language = {en},
	number = {4},
	urldate = {2019-11-29},
	journal = {Trends in Food Science \& Technology},
	author = {Gowen, A. A. and Tiwari, B. K. and Cullen, P. J. and McDonnell, K. and O'Donnell, C. P.},
	month = apr,
	year = {2010},
	keywords = {read},
	pages = {190--200},
	file = {ScienceDirect Full Text PDF:/Users/lindronics/Zotero/storage/IXUHV7PZ/Gowen et al. - 2010 - Applications of thermal imaging in food quality an.pdf:application/pdf;ScienceDirect Snapshot:/Users/lindronics/Zotero/storage/3RKNEFY4/S092422440900301X.html:text/html}
}

@article{ibarra_combined_2000,
	title = {Combined {IR} imaging-neural network method for the estimation of internal temperature in cooked chicken meat},
	volume = {39},
	url = {https://lib.dr.iastate.edu/abe_eng_pubs/208},
	doi = {10.1117/1.1314595},
	number = {11},
	journal = {Optical Engineering},
	author = {Ibarra, Juan and Tao, Yang and Xin, Hongwei},
	month = nov,
	year = {2000},
	keywords = {read},
	pages = {3032--3038},
	file = {"Combined IR imaging-neural network method for the estimation of intern" by Juan G. Ibarra, Yang Tao et al.:/Users/lindronics/Zotero/storage/RC77NMY3/208.html:text/html;Full Text:/Users/lindronics/Zotero/storage/DDR6HPCX/Ibarra et al. - 2000 - Combined IR imaging-neural network method for the .pdf:application/pdf}
}

@inproceedings{lopez_detecting_2017,
	address = {Montreal, QC},
	title = {Detecting exercise-induced fatigue using thermal imaging and deep learning},
	isbn = {978-1-5386-1842-4},
	url = {http://ieeexplore.ieee.org/document/8310151/},
	doi = {10.1109/IPTA.2017.8310151},
	abstract = {Fatigue has adverse eﬀects in both physical and cognitive abilities. Hence, automatically detecting exercise-induced fatigue is of importance, especially in order to assist in the planning of eﬀort and resting during exercise sessions. Thermal imaging and facial analysis provide a mean to detect changes in the human body unobtrusively and in variant conditions of pose and illumination. In this context, this paper proposes the automatic detection of exercise-induced fatigue using thermal cameras and facial images, analyzing them using deep convolutional neural networks. Our results indicate that classiﬁcation of fatigued individuals is possible, obtaining an accuracy that reaches over 80\% when utilizing single thermal images.},
	language = {en},
	urldate = {2019-12-12},
	booktitle = {2017 {Seventh} {International} {Conference} on {Image} {Processing} {Theory}, {Tools} and {Applications} ({IPTA})},
	publisher = {IEEE},
	author = {Lopez, Miguel Bordallo and del-Blanco, Carlos R. and Garcia, Narciso},
	month = nov,
	year = {2017},
	pages = {1--6},
	file = {Lopez et al. - 2017 - Detecting exercise-induced fatigue using thermal i.pdf:/Users/lindronics/Zotero/storage/XQ7X7GDB/Lopez et al. - 2017 - Detecting exercise-induced fatigue using thermal i.pdf:application/pdf}
}

@article{mocanu_deep_2016,
	title = {Deep learning for estimating building energy consumption},
	volume = {6},
	issn = {2352-4677},
	url = {http://www.sciencedirect.com/science/article/pii/S2352467716000163},
	doi = {10.1016/j.segan.2016.02.005},
	abstract = {To improve the design of the electricity infrastructure and the efficient deployment of distributed and renewable energy sources, a new paradigm for the energy supply chain is emerging, leading to the development of smart grids. There is a need to add intelligence at all levels in the grid, acting over various time horizons. Predicting the behavior of the energy system is crucial to mitigate potential uncertainties. An accurate energy prediction at the customer level will reflect directly in efficiency improvements in the whole system. However, prediction of building energy consumption is complex due to many influencing factors, such as climate, performance of thermal systems, and occupancy patterns. Therefore, current state-of-the-art methods are not able to confine the uncertainty at the building level due to the many fluctuations in influencing variables. As an evolution of artificial neural network (ANN)-based prediction methods, deep learning techniques are expected to increase the prediction accuracy by allowing higher levels of abstraction. In this paper, we investigate two newly developed stochastic models for time series prediction of energy consumption, namely Conditional Restricted Boltzmann Machine (CRBM) and Factored Conditional Restricted Boltzmann Machine (FCRBM). The assessment is made on a benchmark dataset consisting of almost four years of one minute resolution electric power consumption data collected from an individual residential customer. The results show that for the energy prediction problem solved here, FCRBM outperforms ANN, Support Vector Machine (SVM), Recurrent Neural Networks (RNN) and CRBM.},
	language = {en},
	urldate = {2019-12-12},
	journal = {Sustainable Energy, Grids and Networks},
	author = {Mocanu, Elena and Nguyen, Phuong H. and Gibescu, Madeleine and Kling, Wil L.},
	month = jun,
	year = {2016},
	keywords = {Artificial Neural Networks, Conditional Restricted Boltzmann Machine, Energy prediction, Factored Conditional Restricted Boltzmann Machine},
	pages = {91--99},
	file = {ScienceDirect Full Text PDF:/Users/lindronics/Zotero/storage/IEZBECEZ/Mocanu et al. - 2016 - Deep learning for estimating building energy consu.pdf:application/pdf;ScienceDirect Snapshot:/Users/lindronics/Zotero/storage/H6YRBSIZ/S2352467716000163.html:text/html}
}

@inproceedings{farhan_predicting_2015,
	title = {Predicting individual thermal comfort using machine learning algorithms},
	doi = {10.1109/CoASE.2015.7294164},
	abstract = {Human thermal sensation in an environment may be delayed, which may lead to life threatening conditions, such as hypothermia and hyperthermia. This is especially true for senior citizens, as aging alters the thermal perception in humans. We envision a decision support system that predicts human thermal comfort in real-time using various environmental conditions as well psychological and physiological features, and suggest corresponding actions, which can significantly improve overall thermal comfort and health of individuals, especially senior citizens. The key to realize this vision is an accurate thermal comfort model. We propose a novel machine learning based approach to learn an individual's thermal comfort model. This approach identifies the best set of features, and then learns a classifier that takes a feature vector as input and outputs a corresponding thermal sensation class (i.e. “feeling cold”, “neutral” and “feeling warm”). Evaluation using a large-scale publicly available data demonstrates that when using Support Vector Machines (SVM) classifiers, the accuracy of our approach is 76.7\%, over two times higher than that of the widely adopted Fanger's model (which only achieves accuracy of 35.4\%). In addition, our study indicates that two factors, a person's age and outdoor temperature that are not included in Fanger's model, play an important role in thermal comfort, which is a finding interesting in its own right.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Automation} {Science} and {Engineering} ({CASE})},
	author = {Farhan, Asma Ahmad and Pattipati, Krishna and Wang, Bing and Luh, Peter},
	month = aug,
	year = {2015},
	note = {ISSN: 2161-8089},
	keywords = {learning (artificial intelligence), Support vector machines, Accuracy, Adaptation models, classifier learning, decision support system, environmental conditions, feature vector, feeling-cold sensation, feeling-warm sensation, human factors, human thermal sensation, large-scale publicly available data, machine learning algorithm, Machine learning algorithms, machine learning based approach, neutral sensation, outdoor temperature, pattern classification, person age, physiological features, physiology, psychological features, psychology, senior citizens, support vector machine classifiers, support vector machines, SVM classifiers, Temperature sensors, thermal comfort prediction, thermal perception},
	pages = {708--713},
	file = {IEEE Xplore Abstract Record:/Users/lindronics/Zotero/storage/SCZQCN67/7294164.html:text/html;IEEE Xplore Full Text PDF:/Users/lindronics/Zotero/storage/V7EI9JTJ/Farhan et al. - 2015 - Predicting individual thermal comfort using machin.pdf:application/pdf}
}

@article{cho_deep_2018,
	title = {Deep {Thermal} {Imaging}: {Proximate} {Material} {Type} {Recognition} in the {Wild} through {Deep} {Learning} of {Spatial} {Surface} {Temperature} {Patterns}},
	shorttitle = {Deep {Thermal} {Imaging}},
	url = {http://arxiv.org/abs/1803.02310},
	doi = {10.1145/3173574.3173576},
	abstract = {We introduce Deep Thermal Imaging, a new approach for close-range automatic recognition of materials to enhance the understanding of people and ubiquitous technologies of their proximal environment. Our approach uses a low-cost mobile thermal camera integrated into a smartphone to capture thermal textures. A deep neural network classifies these textures into material types. This approach works effectively without the need for ambient light sources or direct contact with materials. Furthermore, the use of a deep learning network removes the need to handcraft the set of features for different materials. We evaluated the performance of the system by training it to recognise 32 material types in both indoor and outdoor environments. Our approach produced recognition accuracies above 98\% in 14,860 images of 15 indoor materials and above 89\% in 26,584 images of 17 outdoor materials. We conclude by discussing its potentials for real-time use in HCI applications and future directions.},
	urldate = {2019-12-30},
	journal = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems  - CHI '18},
	author = {Cho, Youngjun and Bianchi-Berthouze, Nadia and Marquardt, Nicolai and Julier, Simon J.},
	year = {2018},
	note = {arXiv: 1803.02310},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Condensed Matter - Materials Science},
	pages = {1--13},
	file = {arXiv Fulltext PDF:/Users/lindronics/Zotero/storage/A4CKYVDT/Cho et al. - 2018 - Deep Thermal Imaging Proximate Material Type Reco.pdf:application/pdf;arXiv.org Snapshot:/Users/lindronics/Zotero/storage/7SC9IU4V/1803.html:text/html}
}

@article{nowruzi_-vehicle_nodate,
	title = {In-{Vehicle} {Occupancy} {Detection} {With} {Convolutional} {Networks} on {Thermal} {Images}},
	abstract = {Counting people is a growing ﬁeld of interest for researchers in recent years. In-vehicle passenger counting is an interesting problem in this domain that has several applications including High Occupancy Vehicle (HOV) lanes. In this paper, present a new in-vehicle thermal image dataset. We propose a tiny convolutional model to count on-board passengers and compare it to well known methods. We show that our model surpasses state-of-the-art methods in classiﬁcation and has comparable performance in detection. Moreover, our model outperforms the state-of-the-art architectures in terms of speed, making it suitable for deployment on embedded platforms. We present the results of multiple deep learning models and thoroughly analyze them.},
	language = {en},
	author = {Nowruzi, Farzan Erlik and Ahmar, Wassim A El and Laganiere, Robert and Ghods, Amir H},
	pages = {8},
	file = {Nowruzi et al. - In-Vehicle Occupancy Detection With Convolutional .pdf:/Users/lindronics/Zotero/storage/CA596UHW/Nowruzi et al. - In-Vehicle Occupancy Detection With Convolutional .pdf:application/pdf}
}

@article{baranowski_detection_2012,
	title = {Detection of early bruises in apples using hyperspectral data and thermal imaging},
	volume = {110},
	issn = {0260-8774},
	url = {http://www.sciencedirect.com/science/article/pii/S0260877412000027},
	doi = {10.1016/j.jfoodeng.2011.12.038},
	abstract = {The early detection of bruises in apples was studied using a system that included hyperspectral cameras equipped with sensors working in the visible and near-infrared (400–1000nm), short wavelength infrared (1000–2500nm) and thermal imaging camera in mid-wavelength infrared (3500–5000nm) ranges. The principal components analysis (PCA) and minimum noise fraction (MNF) analyses of the images that were captured in particular ranges made it possible to distinguish between areas with defects in the tissue and the sound ones. The fast Fourier analysis of the image sequences after pulse heating of the fruit surface provided additional information not only about the position of the area of damaged tissue but also about its depth. The comparison of the results obtained with supervised classification methods, including soft independent modelling of class analogy (SIMCA), linear discriminant analysis (LDA) and support vector machines (SVM) confirmed that broad spectrum range (400–5000nm) of fruit surface imaging can improve the detection of early bruises with varying depths.},
	language = {en},
	number = {3},
	urldate = {2020-01-04},
	journal = {Journal of Food Engineering},
	author = {Baranowski, Piotr and Mazurek, Wojciech and Wozniak, Joanna and Majewska, Urszula},
	month = jun,
	year = {2012},
	keywords = {Thermal imaging, Hyperspectral imaging, Apples and bruise},
	pages = {345--355},
	file = {ScienceDirect Full Text PDF:/Users/lindronics/Zotero/storage/IX8DTY8I/Baranowski et al. - 2012 - Detection of early bruises in apples using hypersp.pdf:application/pdf;ScienceDirect Snapshot:/Users/lindronics/Zotero/storage/YSDSHR5T/S0260877412000027.html:text/html}
}

@inproceedings{abdul_multi-disnet_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Multi-{DisNet}: {Machine} {Learning}-{Based} {Object} {Distance} {Estimation} from {Multiple} {Cameras}},
	isbn = {978-3-030-34995-0},
	shorttitle = {Multi-{DisNet}},
	doi = {10.1007/978-3-030-34995-0_41},
	abstract = {In this paper, a novel method for distance estimation from multiple cameras to the object viewed with these cameras is presented. The core element of the method is multilayer neural network named Multi-DisNet, which is used to learn the relationship between the sizes of the object bounding boxes in the cameras images and the distance between the object and the cameras. The Multi-DisNet was trained using a supervised learning technique where the input features were manually calculated parameters of the objects bounding boxes in the cameras images and outputs were ground-truth distances between the objects and the cameras. The presented distance estimation system can be of benefit for all applications where object (obstacle) distance estimation is essential for the safety such as autonomous driving applications in automotive or railway. The presented object distance estimation system was evaluated on the images of real-world railway scenes. As a proof-of-concept, the results on the fusion of two sensors, an RGB and thermal camera mounted on a moving train, in the Multi-DisNet distance estimation system are shown. Shown results demonstrate both the good performance of Multi-DisNet system to estimate the mid (up to 200 m) and long-range (up to 1000 m) object distance and benefit of sensor fusion to overcome the problem of not reliable object detection.},
	language = {en},
	booktitle = {Computer {Vision} {Systems}},
	publisher = {Springer International Publishing},
	author = {Abdul, Haseeb Muhammad and Danijela, Ristić-Durrant and Axel, Gräser and Milan, Banić and Dušan, Stamenković},
	editor = {Tzovaras, Dimitrios and Giakoumis, Dimitrios and Vincze, Markus and Argyros, Antonis},
	year = {2019},
	keywords = {Machine learning, Autonomous obstacle detection for railways, Sensor fusion},
	pages = {457--469},
	file = {Springer Full Text PDF:/Users/lindronics/Zotero/storage/VZ32RPHW/Abdul et al. - 2019 - Multi-DisNet Machine Learning-Based Object Distan.pdf:application/pdf}
}

@article{chen_normalized_2018,
	title = {Normalized {Total} {Gradient}: {A} {New} {Measure} for {Multispectral} {Image} {Registration}},
	volume = {27},
	issn = {1057-7149, 1941-0042},
	shorttitle = {Normalized {Total} {Gradient}},
	url = {http://arxiv.org/abs/1702.04562},
	doi = {10.1109/TIP.2017.2776753},
	abstract = {Image registration is a fundamental issue in multispectral image processing. In filter wheel based multispectral imaging systems, the non-coplanar placement of the filters always causes the misalignment of multiple channel images. The selective characteristic of spectral response in multispectral imaging raises two challenges to image registration. First, the intensity levels of a local region may be different in individual channel images. Second, the local intensity may vary rapidly in some channel images while keeps stationary in others. Conventional multimodal measures, such as mutual information, correlation coefficient, and correlation ratio, can register images with different regional intensity levels, but will fail in the circumstance of severe local intensity variation. In this paper, a new measure, namely normalized total gradient (NTG), is proposed for multispectral image registration. The NTG is applied on the difference between two channel images. This measure is based on the key assumption (observation) that the gradient of difference image between two aligned channel images is sparser than that between two misaligned ones. A registration framework, which incorporates image pyramid and global/local optimization, is further introduced for rigid transform. Experimental results validate that the proposed method is effective for multispectral image registration and performs better than conventional methods.},
	number = {3},
	urldate = {2020-01-27},
	journal = {IEEE Transactions on Image Processing},
	author = {Chen, Shu-Jie and Shen, Hui-Liang},
	month = mar,
	year = {2018},
	note = {arXiv: 1702.04562},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {1297--1310},
	file = {arXiv Fulltext PDF:/Users/lindronics/Zotero/storage/J5KS947E/Chen and Shen - 2018 - Normalized Total Gradient A New Measure for Multi.pdf:application/pdf;arXiv.org Snapshot:/Users/lindronics/Zotero/storage/MAKSZZIL/1702.html:text/html}
}

@inproceedings{istenic_thermal_2007,
	title = {Thermal and {Visual} {Image} {Registration} in {Hough} {Parameter} {Space}},
	isbn = {978-961-248-029-5},
	doi = {10.1109/IWSSIP.2007.4381164},
	abstract = {This paper introduces a new approach in image registration, that is a multisensor registration in Hough parameter space. Visual and thermal images of building fronts were aimed to be aligned in order to inspect thermal properties of buildings. Some preprocessing of visible images was necessary to be comparable to their thermal counterparts, namely downsampling and color space conversion from RGB to grayscale intensity. For each image pair, edges were detected with Canny edge detector and, as a result, binary edge images were obtained. These images were further processed by Hough transform which extracted all linear image segments. We decided for linear segments, because they are the most frequent feature appearing in the images of buildings. In the Hough parameter space the rotation and translation of the linear segments can be recovered using the line correspondence analysis. The method was verified first on synthetic images with only translation, only rotation, and also both the rotation and translation together. Finally, a verification on real images was done. The method was able to correctly register both type of images, synthetic and the real ones. In general, our algorithm can cope with rotated and translated images if only a few linear segments are detectible.},
	author = {Istenic, R. and Heric, D. and Ribaric, S. and Zazula, Damjan},
	month = jul,
	year = {2007},
	pages = {106--109},
	file = {Full Text PDF:/Users/lindronics/Zotero/storage/6XT5E6WY/Istenic et al. - 2007 - Thermal and Visual Image Registration in Hough Par.pdf:application/pdf}
}

@article{myronenko_intensity-based_2010,
	title = {Intensity-{Based} {Image} {Registration} by {Minimizing} {Residual} {Complexity}},
	volume = {29},
	issn = {1558-254X},
	doi = {10.1109/TMI.2010.2053043},
	abstract = {Accurate definition of the similarity measure is a key component in image registration. Most commonly used intensity-based similarity measures rely on the assumptions of independence and stationarity of the intensities from pixel to pixel. Such measures cannot capture the complex interactions among the pixel intensities, and often result in less satisfactory registration performances, especially in the presence of spatially-varying intensity distortions. We propose a novel similarity measure that accounts for intensity nonstationarities and complex spatially-varying intensity distortions in mono-modal settings. We derive the similarity measure by analytically solving for the intensity correction field and its adaptive regularization. The final measure can be interpreted as one that favors a registration with minimum compression complexity of the residual image between the two registered images. One of the key advantages of the new similarity measure is its simplicity in terms of both computational complexity and implementation. This measure produces accurate registration results on both artificial and real-world problems that we have tested, and outperforms other state-of-the-art similarity measures in these cases.},
	number = {11},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Myronenko, Andriy and Song, Xubo},
	month = nov,
	year = {2010},
	keywords = {adaptive regularization, Algorithms, Artificial Intelligence, Bias field, Biological materials, Biomedical materials, Biomedical measurements, Brain, compression complexity, computational complexity, Computational complexity, Distortion measurement, Humans, image coding, Image coding, Image Enhancement, Image Interpretation, Computer-Assisted, image registration, Image registration, intensity correction field, intensity-based similarity measures, Magnetic Resonance Imaging, medical image processing, nonstationary intensity distortion, Pattern Recognition, Automated, Performance evaluation, Permission, Pixel, Reproducibility of Results, residual complexity, Sensitivity and Specificity, sparseness, spatially-varying intensity distortions, Subtraction Technique},
	pages = {1882--1891},
	file = {IEEE Xplore Abstract Record:/Users/lindronics/Zotero/storage/GUF6BIGJ/5487419.html:text/html;IEEE Xplore Full Text PDF:/Users/lindronics/Zotero/storage/DRUD7UJ6/Myronenko and Song - 2010 - Intensity-Based Image Registration by Minimizing R.pdf:application/pdf}
}

@inproceedings{mikolajczyk_data_2018,
	title = {Data augmentation for improving deep learning in image classification problem},
	doi = {10.1109/IIPHDW.2018.8388338},
	abstract = {These days deep learning is the fastest-growing field in the field of Machine Learning (ML) and Deep Neural Networks (DNN). Among many of DNN structures, the Convolutional Neural Networks (CNN) are currently the main tool used for the image analysis and classification purposes. Although great achievements and perspectives, deep neural networks and accompanying learning algorithms have some relevant challenges to tackle. In this paper, we have focused on the most frequently mentioned problem in the field of machine learning, that is the lack of sufficient amount of the training data or uneven class balance within the datasets. One of the ways of dealing with this problem is so called data augmentation. In the paper we have compared and analyzed multiple methods of data augmentation in the task of image classification, starting from classical image transformations like rotating, cropping, zooming, histogram based methods and finishing at Style Transfer and Generative Adversarial Networks, along with the representative examples. Next, we presented our own method of data augmentation based on image style transfer. The method allows to generate the new images of high perceptual quality that combine the content of a base image with the appearance of another ones. The newly created images can be used to pre-train the given neural network in order to improve the training process efficiency. Proposed method is validated on the three medical case studies: skin melanomas diagnosis, histopathological images and breast magnetic resonance imaging (MRI) scans analysis, utilizing the image classification in order to provide a diagnose. In such kind of problems the data deficiency is one of the most relevant issues. Finally, we discuss the advantages and disadvantages of the methods being analyzed.},
	booktitle = {2018 {International} {Interdisciplinary} {PhD} {Workshop} ({IIPhDW})},
	author = {Mikołajczyk, Agnieszka and Grochowski, Michał},
	month = may,
	year = {2018},
	note = {ISSN: null},
	keywords = {image classification, Machine learning, learning (artificial intelligence), Task analysis, deep learning, neural nets, medical image processing, biomedical MRI, breast magnetic resonance imaging scans analysis, Cancer, CNN, convolution, convolutional neural networks, data augmentation, deep neural networks, DNN structures, histopathological images, image analysis, Image classification, image classification problem, Image color analysis, image style transfer, learning algorithms, Lesions, medical imaging, ML, MRI, Neural networks, skin, skin melanomas diagnosis, style transfer, training data},
	pages = {117--122},
	file = {IEEE Xplore Abstract Record:/Users/lindronics/Zotero/storage/IZPQ6RPT/8388338.html:text/html;IEEE Xplore Full Text PDF:/Users/lindronics/Zotero/storage/3EF4E7PI/Mikołajczyk and Grochowski - 2018 - Data augmentation for improving deep learning in i.pdf:application/pdf}
}

@book{jarc_graz_2007,
	title = {Graz {Technical} {University} {Texture} features for affine registration of thermal ({FLIR}) and visible images},
	abstract = {Abstract The aim of our research is to analyse the importance of texture information for affine registration of gray-scale far-infrared (FLIR) images and gray-scale images taken in the visible spectrum. Texture features are extracted by Laws texture coefficients and used for computing registration criterion functions. The proposed feature based approach is compared to the commonly used approach, where a registration criterion function is computed directly from intensity features, i.e. grey values. The experiments on a small database of ten image pairs show that our texture feature based registration method works well for the given image modalities. Furthermore, we can expect more robust and more correct registration when texture based criterion function instead of intensity based one is used. 1 Introduction and},
	author = {Jarc, Andreja and Rogelj, Peter and Kovačič, Stanislav},
	year = {2007},
	file = {Citeseer - Full Text PDF:/Users/lindronics/Zotero/storage/ZMW7CAVW/Jarc et al. - Graz Technical University Texture features for aff.pdf:application/pdf;Citeseer - Snapshot:/Users/lindronics/Zotero/storage/G7E83UXI/summary.html:text/html}
}

@inproceedings{laws_rapid_1980,
	title = {Rapid {Texture} {Identification}},
	volume = {0238},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/0238/0000/Rapid-Texture-Identification/10.1117/12.959169.short},
	doi = {10.1117/12.959169},
	abstract = {A method is presented for classifying each pixel of a textured image, and thus for segmenting the scene. The \&quot;texture energy\&quot; approach requires only a few convolutions with small (typically 5x5) integer coefficient masks, followed by a moving-window absolute average operation. Normalization by the local mean and standard deviation eliminates the need for histogram equalization. Rotation-invariance can also be achieved by using averages of the texture energy features. The convolution masks are separable, and can be implemented with 1-dimensional (vertical and horizontal) or multipass 3x3 convolutions. Special techniques permit rapid processing on general-purpose digital computers.},
	urldate = {2020-02-05},
	booktitle = {Image {Processing} for {Missile} {Guidance}},
	publisher = {International Society for Optics and Photonics},
	author = {Laws, Kenneth I.},
	month = dec,
	year = {1980},
	pages = {376--381},
	file = {Full Text PDF:/Users/lindronics/Zotero/storage/32RZ3JU3/Laws - 1980 - Rapid Texture Identification.pdf:application/pdf;Snapshot:/Users/lindronics/Zotero/storage/VW4FG33Q/12.959169.html:text/html}
}

@article{qin_hyperspectral_2013,
	title = {Hyperspectral and multispectral imaging for evaluating food safety and quality},
	volume = {118},
	issn = {0260-8774},
	url = {http://www.sciencedirect.com/science/article/pii/S0260877413001659},
	doi = {10.1016/j.jfoodeng.2013.04.001},
	abstract = {Spectral imaging technologies have been developed rapidly during the past decade. This paper presents hyperspectral and multispectral imaging technologies in the area of food safety and quality evaluation, with an introduction, demonstration, and summarization of current spectral imaging techniques available to the food industry for practical commercial use. The main topics include methods for acquiring spectral images, components for building spectral imaging systems, methods for calibrating spectral imaging systems, and techniques for analyzing spectral images. The applications for evaluating food and agricultural products are presented to reflect common practices of the spectral imaging techniques. Future development of hyperspectral and multispectral imaging is also discussed.},
	language = {en},
	number = {2},
	urldate = {2020-02-10},
	journal = {Journal of Food Engineering},
	author = {Qin, Jianwei and Chao, Kuanglin and Kim, Moon S. and Lu, Renfu and Burks, Thomas F.},
	month = sep,
	year = {2013},
	keywords = {Food quality, Food safety, Hyperspectral, Machine vision, Multispectral, Nondestructive sensing},
	pages = {157--171},
	file = {ScienceDirect Full Text PDF:/Users/lindronics/Zotero/storage/2JL3D96S/Qin et al. - 2013 - Hyperspectral and multispectral imaging for evalua.pdf:application/pdf;ScienceDirect Snapshot:/Users/lindronics/Zotero/storage/DR3PH4WW/S0260877413001659.html:text/html}
}

@article{dale_hyperspectral_2013,
	title = {Hyperspectral {Imaging} {Applications} in {Agriculture} and {Agro}-{Food} {Product} {Quality} and {Safety} {Control}: {A} {Review}},
	volume = {48},
	issn = {0570-4928},
	shorttitle = {Hyperspectral {Imaging} {Applications} in {Agriculture} and {Agro}-{Food} {Product} {Quality} and {Safety} {Control}},
	url = {https://doi.org/10.1080/05704928.2012.705800},
	doi = {10.1080/05704928.2012.705800},
	abstract = {In this review, various applications of near-infrared hyperspectral imaging (NIR-HSI) in agriculture and in the quality control of agro-food products are presented. NIR-HSI is an emerging technique that combines classical NIR spectroscopy and imaging techniques in order to simultaneously obtain spectral and spatial information from a field or a sample. The technique is nondestructive, nonpolluting, fast, and relatively inexpensive per analysis. Currently, its applications in agriculture include vegetation mapping, crop disease, stress and yield detection, component identification in plants, and detection of impurities. There is growing interest in HSI for safety and quality assessments of agro-food products. The applications have been classified from the level of satellite images to the macroscopic or molecular level.},
	number = {2},
	urldate = {2020-02-10},
	journal = {Applied Spectroscopy Reviews},
	author = {Dale, Laura M. and Thewis, André and Boudry, Christelle and Rotar, Ioan and Dardenne, Pierre and Baeten, Vincent and Pierna, Juan A. Fernández},
	month = mar,
	year = {2013},
	keywords = {agriculture, agro-food industry, airborne system, ground-based HSI, NIR spectroscopy, NIR-HSI, satellite system},
	pages = {142--159},
	file = {Snapshot:/Users/lindronics/Zotero/storage/WIUZY85V/05704928.2012.html:text/html}
}

@article{lu_medical_2014,
	title = {Medical hyperspectral imaging: a review},
	volume = {19},
	issn = {1083-3668, 1560-2281},
	shorttitle = {Medical hyperspectral imaging},
	url = {https://www.spiedigitallibrary.org/journals/Journal-of-Biomedical-Optics/volume-19/issue-1/010901/Medical-hyperspectral-imaging-a-review/10.1117/1.JBO.19.1.010901.short},
	doi = {10.1117/1.JBO.19.1.010901},
	abstract = {Hyperspectral imaging (HSI) is an emerging imaging modality for medical applications, especially in disease diagnosis and image-guided surgery. HSI acquires a three-dimensional dataset called hypercube, with two spatial dimensions and one spectral dimension. Spatially resolved spectral imaging obtained by HSI provides diagnostic information about the tissue physiology, morphology, and composition. This review paper presents an overview of the literature on medical hyperspectral imaging technology and its applications. The aim of the survey is threefold: an introduction for those new to the field, an overview for those working in the field, and a reference for those searching for literature on a specific application.},
	number = {1},
	urldate = {2020-02-10},
	journal = {Journal of Biomedical Optics},
	author = {Lu, Guolan and Fei, Baowei},
	month = jan,
	year = {2014},
	pages = {010901},
	file = {Full Text PDF:/Users/lindronics/Zotero/storage/3YMDM9IY/Lu and Fei - 2014 - Medical hyperspectral imaging a review.pdf:application/pdf;Snapshot:/Users/lindronics/Zotero/storage/YUL6WWK2/1.JBO.19.1.010901.html:text/html}
}

@book{byrnes_unexploded_2008,
	title = {Unexploded {Ordnance} {Detection} and {Mitigation}},
	isbn = {978-1-4020-9253-4},
	abstract = {The chapters in this volume were presented at the July–August 2008 NATO Advanced Study Institute on Unexploded Ordnance Detection and Mitigation. The conference was held at the beautiful Il Ciocco resort near Lucca, in the glorious Tuscany region of northern Italy. For the ninth time we gathered at this idyllic spot to explore and extend the reciprocity between mathematics and engineering. The dynamic interaction between world-renowned scientists from the usually disparate communities of pure mathematicians and applied scientists which occurred at our eight previous ASI’s continued at this meeting. The detection and neutralization of unexploded ordnance (UXO) has been of major concern for very many decades; at least since the First World war. UXO continues to be the subject of intensive research in many ?elds of science, incl- ing mathematics, signal processing (mainly radar and sonar) and chemistry. While today’s headlines emphasize the mayhem resulting from the placement of imp- vised explosive devices (IEDs), humanitarian landmine clearing continues to draw signi?cant global attention as well. In many countries of the world, landmines threaten the population and hinder reconstruction and fast, ef?cient utilization of large areas of the mined land in the aftermath of military con?icts.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Byrnes, James},
	month = dec,
	year = {2008},
	note = {Google-Books-ID: B\_KLtHJbgvAC},
	keywords = {Computers / Computer Science, Computers / Computer Vision \& Pattern Recognition, Computers / Optical Data Processing, Mathematics / Applied, Science / Chemistry / Analytic, Science / Earth Sciences / Geography, Technology \& Engineering / Microwaves, Technology \& Engineering / Remote Sensing \& Geographic Information Systems}
}

@article{kogure_thermodynamic_2007,
	title = {Thermodynamic equilibrium and black-body radiation},
	journal = {The astrophysics of emission-line stars. Springer. p41},
	author = {Kogure, Tomokazu and Leung, Kam-Ching},
	year = {2007}
}

@book{young_sears_2012,
	edition = {13th},
	title = {Sears and {Zemansky}'s university physics: with modern physics},
	publisher = {Pearson Addison-Wesley, San Francisco},
	author = {Young, H.D. and Freedman, R.A. and Ford, A.L. and Sears, F.W. and Zemansky, M.W.},
	year = {2012}
}

@book{burkov_hundred-page_2019,
	address = {Quebec, Canada},
	title = {The hundred-page machine learning book},
	publisher = {Andriy Burkov},
	author = {Burkov, Andriy},
	year = {2019}
}

@article{dyk_art_2001,
	title = {The {Art} of {Data} {Augmentation}},
	volume = {10},
	issn = {1061-8600},
	url = {https://doi.org/10.1198/10618600152418584},
	doi = {10.1198/10618600152418584},
	abstract = {The term data augmentation refers to methods for constructing iterative optimization or sampling algorithms via the introduction of unobserved data or latent variables. For deterministic algorithms, the method was popularized in the general statistical community by the seminal article by Dempster, Laird, and Rubin on the EM algorithm for maximizing a likelihood function or, more generally, a posterior density. For stochastic algorithms, the method was popularized in the statistical literature by Tanner and Wong's Data Augmentation algorithm for posterior sampling and in the physics literature by Swendsen and Wang's algorithm for sampling from the Ising and Potts models and their generalizations; in the physics literature, the method of data augmentation is referred to as the method of auxiliary variables. Data augmentation schemes were used by Tanner and Wong to make simulation feasible and simple, while auxiliary variables were adopted by Swendsen and Wang to improve the speed of iterative simulation. In general, however, constructing data augmentation schemes that result in both simple and fast algorithms is a matter of art in that successful strategies vary greatly with the (observed-data) models being considered. After an overview of data augmentation/auxiliary variables and some recent developments in methods for constructing such efficient data augmentation schemes, we introduce an effective search strategy that combines the ideas of marginal augmentation and conditional augmentation, together with a deterministic approximation method for selecting good augmentation schemes. We then apply this strategy to three common classes of models (specifically, multivariate t, probit regression, and mixed-effects models) to obtain efficient Markov chain Monte Carlo algorithms for posterior sampling. We provide theoretical and empirical evidence that the resulting algorithms, while requiring similar programming effort, can show dramatic improvement over the Gibbs samplers commonly used for these models in practice. A key feature of all these new algorithms is that they are positive recurrent subchains of nonpositive recurrent Markov chains constructed in larger spaces.},
	number = {1},
	urldate = {2020-02-18},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Dyk, David A. van and Meng, Xiao-Li},
	month = mar,
	year = {2001},
	keywords = {Auxiliary variables, Conditional augmentation, Em algorithm, Gibbs sampler, Haar measure, Hierarchical models, Marginal augmentation, Markov chain Monte Carlo, Mixed-effects models, Nonpositive recurrent markov chain, Posterior distributions, Probit regression, Rate of convergence},
	pages = {1--50},
	file = {Full Text PDF:/Users/lindronics/Zotero/storage/5MHAQ5GC/Dyk and Meng - 2001 - The Art of Data Augmentation.pdf:application/pdf;Snapshot:/Users/lindronics/Zotero/storage/XH7CN4M7/10618600152418584.html:text/html}
}

@article{perez_effectiveness_2017,
	title = {The {Effectiveness} of {Data} {Augmentation} in {Image} {Classification} using {Deep} {Learning}},
	url = {http://arxiv.org/abs/1712.04621},
	abstract = {In this paper, we explore and compare multiple solutions to the problem of data augmentation in image classification. Previous work has demonstrated the effectiveness of data augmentation through simple techniques, such as cropping, rotating, and flipping input images. We artificially constrain our access to data to a small subset of the ImageNet dataset, and compare each data augmentation technique in turn. One of the more successful data augmentations strategies is the traditional transformations mentioned above. We also experiment with GANs to generate images of different styles. Finally, we propose a method to allow a neural net to learn augmentations that best improve the classifier, which we call neural augmentation. We discuss the successes and shortcomings of this method on various datasets.},
	urldate = {2020-02-18},
	journal = {arXiv:1712.04621 [cs]},
	author = {Perez, Luis and Wang, Jason},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.04621},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/lindronics/Zotero/storage/AUJX3QT6/Perez and Wang - 2017 - The Effectiveness of Data Augmentation in Image Cl.pdf:application/pdf;arXiv.org Snapshot:/Users/lindronics/Zotero/storage/3XPR5JY6/1712.html:text/html}
}

@inproceedings{fawzi_adaptive_2016,
	title = {Adaptive data augmentation for image classification},
	doi = {10.1109/ICIP.2016.7533048},
	abstract = {Data augmentation is the process of generating samples by transforming training data, with the target of improving the accuracy and robustness of classifiers. In this paper, we propose a new automatic and adaptive algorithm for choosing the transformations of the samples used in data augmentation. Specifically, for each sample, our main idea is to seek a small transformation that yields maximal classification loss on the transformed sample. We employ a trust-region optimization strategy, which consists of solving a sequence of linear programs. Our data augmentation scheme is then integrated into a Stochastic Gradient Descent algorithm for training deep neural networks. We perform experiments on two datasets, and show that that the proposed scheme outperforms random data augmentation algorithms in terms of accuracy and robustness, while yielding comparable or superior results with respect to existing selective sampling approaches.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Fawzi, Alhussein and Samulowitz, Horst and Turaga, Deepak and Frossard, Pascal},
	month = sep,
	year = {2016},
	note = {ISSN: 2381-8549},
	keywords = {image classification, Training, neural nets, deep neural networks, Neural networks, adaptive data augmentation, Approximation algorithms, Data augmentation, data handling, gradient methods, image robustness, linear program sequence, linear programming, Optimization, Robustness, stochastic gradient descent algorithm, stochastic processes, Training data, transformation invariance, Transforms, trust-region optimization, trust-region optimization strategy},
	pages = {3688--3692},
	file = {IEEE Xplore Abstract Record:/Users/lindronics/Zotero/storage/U6EMXIKN/7533048.html:text/html;IEEE Xplore Full Text PDF:/Users/lindronics/Zotero/storage/BJC2SVLX/Fawzi et al. - 2016 - Adaptive data augmentation for image classificatio.pdf:application/pdf}
}

@article{ronneberger_u-net_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	urldate = {2020-02-20},
	journal = {arXiv:1505.04597 [cs]},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	note = {arXiv: 1505.04597},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/lindronics/Zotero/storage/RQ5NLFSL/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf:application/pdf;arXiv.org Snapshot:/Users/lindronics/Zotero/storage/3RVGEMIP/1505.html:text/html}
}

@inproceedings{hwang_multispectral_2015,
	title = {Multispectral {Pedestrian} {Detection}: {Benchmark} {Dataset} and {Baseline}},
	shorttitle = {Multispectral {Pedestrian} {Detection}},
	url = {http://openaccess.thecvf.com/content_cvpr_2015/html/Hwang_Multispectral_Pedestrian_Detection_2015_CVPR_paper.html},
	urldate = {2020-02-22},
	author = {Hwang, Soonmin and Park, Jaesik and Kim, Namil and Choi, Yukyung and So Kweon, In},
	year = {2015},
	pages = {1037--1045},
	file = {Full Text PDF:/Users/lindronics/Zotero/storage/WP24UQMN/Hwang et al. - 2015 - Multispectral Pedestrian Detection Benchmark Data.pdf:application/pdf;Snapshot:/Users/lindronics/Zotero/storage/QWGGLEWG/Hwang_Multispectral_Pedestrian_Detection_2015_CVPR_paper.html:text/html}
}

@inproceedings{deng_imagenet_2009,
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	shorttitle = {{ImageNet}},
	doi = {10.1109/CVPR.2009.5206848},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
	month = jun,
	year = {2009},
	note = {ISSN: 1063-6919},
	keywords = {Robustness, computer vision, Explosions, Image databases, image resolution, image retrieval, Image retrieval, ImageNet database, Information retrieval, Internet, large-scale hierarchical image database, large-scale ontology, Large-scale systems, multimedia computing, multimedia data, Multimedia databases, Ontologies, ontologies (artificial intelligence), Spine, subtree, trees (mathematics), very large databases, visual databases, wordNet structure},
	pages = {248--255},
	file = {IEEE Xplore Abstract Record:/Users/lindronics/Zotero/storage/3YCQKPSB/5206848.html:text/html;Submitted Version:/Users/lindronics/Zotero/storage/WSFQ5F4L/Deng et al. - 2009 - ImageNet A large-scale hierarchical image databas.pdf:application/pdf}
}

@incollection{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
	urldate = {2020-02-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	pages = {1097--1105},
	file = {NIPS Full Text PDF:/Users/lindronics/Zotero/storage/V6GD2IWR/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf:application/pdf;NIPS Snapshot:/Users/lindronics/Zotero/storage/TEF4K5YL/4824-imagenet-classification-with-deep-convolutional-neural-networks.html:text/html}
}

@inproceedings{he_deep_2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
	urldate = {2020-02-22},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	pages = {770--778},
	file = {Full Text PDF:/Users/lindronics/Zotero/storage/282LNTN5/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:application/pdf;Snapshot:/Users/lindronics/Zotero/storage/C4GYBANZ/He_Deep_Residual_Learning_CVPR_2016_paper.html:text/html}
}

@article{ross_information_2003,
	series = {Audio- and {Video}-based {Biometric} {Person} {Authentication} ({AVBPA} 2001)},
	title = {Information fusion in biometrics},
	volume = {24},
	issn = {0167-8655},
	url = {http://www.sciencedirect.com/science/article/pii/S0167865503000795},
	doi = {10.1016/S0167-8655(03)00079-5},
	abstract = {User verification systems that use a single biometric indicator often have to contend with noisy sensor data, restricted degrees of freedom, non-universality of the biometric trait and unacceptable error rates. Attempting to improve the performance of individual matchers in such situations may not prove to be effective because of these inherent problems. Multibiometric systems seek to alleviate some of these drawbacks by providing multiple evidences of the same identity. These systems help achieve an increase in performance that may not be possible using a single biometric indicator. Further, multibiometric systems provide anti-spoofing measures by making it difficult for an intruder to spoof multiple biometric traits simultaneously. However, an effective fusion scheme is necessary to combine the information presented by multiple domain experts. This paper addresses the problem of information fusion in biometric verification systems by combining information at the matching score level. Experimental results on combining three biometric modalities (face, fingerprint and hand geometry) are presented.},
	language = {en},
	number = {13},
	urldate = {2020-02-23},
	journal = {Pattern Recognition Letters},
	author = {Ross, Arun and Jain, Anil},
	month = sep,
	year = {2003},
	keywords = {Biometrics, Decision tree, Face, Fingerprints, Hand geometry, Linear discriminant analysis, Multimodal, Sum rule, Verification},
	pages = {2115--2125},
	file = {ScienceDirect Full Text PDF:/Users/lindronics/Zotero/storage/YDQ9Q6VS/Ross and Jain - 2003 - Information fusion in biometrics.pdf:application/pdf;ScienceDirect Snapshot:/Users/lindronics/Zotero/storage/758QPM6H/S0167865503000795.html:text/html}
}

@article{wu_towards_2015,
	title = {Towards {Dropout} {Training} for {Convolutional} {Neural} {Networks}},
	volume = {71},
	issn = {08936080},
	url = {http://arxiv.org/abs/1512.00242},
	doi = {10.1016/j.neunet.2015.07.007},
	abstract = {Recently, dropout has seen increasing use in deep learning. For deep convolutional neural networks, dropout is known to work well in fully-connected layers. However, its effect in convolutional and pooling layers is still not clear. This paper demonstrates that max-pooling dropout is equivalent to randomly picking activation based on a multinomial distribution at training time. In light of this insight, we advocate employing our proposed probabilistic weighted pooling, instead of commonly used max-pooling, to act as model averaging at test time. Empirical evidence validates the superiority of probabilistic weighted pooling. We also empirically show that the effect of convolutional dropout is not trivial, despite the dramatically reduced possibility of over-fitting due to the convolutional architecture. Elaborately designing dropout training simultaneously in max-pooling and fully-connected layers, we achieve state-of-the-art performance on MNIST, and very competitive results on CIFAR-10 and CIFAR-100, relative to other approaches without data augmentation. Finally, we compare max-pooling dropout and stochastic pooling, both of which introduce stochasticity based on multinomial distributions at pooling stage.},
	urldate = {2020-02-26},
	journal = {Neural Networks},
	author = {Wu, Haibing and Gu, Xiaodong},
	month = nov,
	year = {2015},
	note = {arXiv: 1512.00242},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	pages = {1--10},
	file = {arXiv Fulltext PDF:/Users/lindronics/Zotero/storage/E7JRDLFS/Wu and Gu - 2015 - Towards Dropout Training for Convolutional Neural .pdf:application/pdf;arXiv.org Snapshot:/Users/lindronics/Zotero/storage/SX8CWNWR/1512.html:text/html}
}

@inproceedings{kwasniewska_deep_2017,
	title = {Deep features class activation map for thermal face detection and tracking},
	doi = {10.1109/HSI.2017.8004993},
	abstract = {Recently, capabilities of many computer vision tasks have significantly improved due to advances in Convolutional Neural Networks. In our research, we demonstrate that it can be also used for face detection from low resolution thermal images, acquired with a portable camera. The physical size of the camera used in our research allows for embedding it in a wearable device or indoor remote monitoring solution for elderly and disabled people. The benefits of the proposed architecture were experimentally verified on the thermal video sequences, acquired in various scenarios to address possible limitations of remote diagnostics: movements of the person performing a diagnose and movements of the examined person. The achieved short processing time (42.05±0.21ms) along with high model accuracy (false positives -0.43\%; true positives for the patient focused on a certain task -89.2\%) clearly indicates that the current state of the art in the area of image classification and face tracking in thermography was significantly outperformed.},
	booktitle = {2017 10th {International} {Conference} on {Human} {System} {Interactions} ({HSI})},
	author = {Kwaśniewska, Alicja and Rumiński, Jacek and Rad, Paul},
	month = jul,
	year = {2017},
	note = {ISSN: null},
	keywords = {convolutional neural network, Feature extraction, image classification, Machine learning, Training, infrared imaging, deep learning, neural nets, Adaptation models, convolutional neural networks, Neural networks, computer vision, Face, class activation map, computer vision tasks, deep features class activation map, face detection, Face detection, face localization, inception model, low resolution thermal images, object detection, object tracking, thermal face detection, thermal face tracking, thermography, transfer learning},
	pages = {41--47},
	file = {IEEE Xplore Abstract Record:/Users/lindronics/Zotero/storage/28CFBH7Y/8004993.html:text/html;IEEE Xplore Full Text PDF:/Users/lindronics/Zotero/storage/WRZ9MS2W/Kwaśniewska et al. - 2017 - Deep features class activation map for thermal fac.pdf:application/pdf}
}

@incollection{thrun_is_1996,
	title = {Is {Learning} {The} n-th {Thing} {Any} {Easier} {Than} {Learning} {The} {First}?},
	url = {http://papers.nips.cc/paper/1034-is-learning-the-n-th-thing-any-easier-than-learning-the-first.pdf},
	urldate = {2020-03-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 8},
	publisher = {MIT Press},
	author = {Thrun, Sebastian},
	editor = {Touretzky, D. S. and Mozer, M. C. and Hasselmo, M. E.},
	year = {1996},
	pages = {640--646},
	file = {NIPS Full Text PDF:/Users/lindronics/Zotero/storage/8MERI4WK/Thrun - 1996 - Is Learning The n-th Thing Any Easier Than Learnin.pdf:application/pdf;NIPS Snapshot:/Users/lindronics/Zotero/storage/CQL8J8DM/1034-is-learning-the-n-th-thing-any-easier-than-learning-the-first.html:text/html}
}

@inproceedings{chappelow_improving_2008,
	title = {Improving supervised classification accuracy using non-rigid multimodal image registration: detecting prostate cancer},
	volume = {6915},
	shorttitle = {Improving supervised classification accuracy using non-rigid multimodal image registration},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/6915/69150V/Improving-supervised-classification-accuracy-using-non-rigid-multimodal-image-registration/10.1117/12.770703.short},
	doi = {10.1117/12.770703},
	abstract = {Computer-aided diagnosis (CAD) systems for the detection of cancer in medical images require precise labeling of training data. For magnetic resonance (MR) imaging (MRI) of the prostate, training labels define the spatial extent of prostate cancer (CaP); the most common source for these labels is expert segmentations. When ancillary data such as whole mount histology (WMH) sections, which provide the gold standard for cancer ground truth, are available, the manual labeling of CaP can be improved by referencing WMH. However, manual segmentation is error prone, time consuming and not reproducible. Therefore, we present the use of multimodal image registration to automatically and accurately transcribe CaP from histology onto MRI following alignment of the two modalities, in order to improve the quality of training data and hence classifier performance. We quantitatively demonstrate the superiority of this registration-based methodology by comparing its results to the manual CaP annotation of expert radiologists. Five supervised CAD classifiers were trained using the labels for CaP extent on MRI obtained by the expert and 4 different registration techniques. Two of the registration methods were affi;ne schemes; one based on maximization of mutual information (MI) and the other method that we previously developed, Combined Feature Ensemble Mutual Information (COFEMI), which incorporates high-order statistical features for robust multimodal registration. Two non-rigid schemes were obtained by succeeding the two affine registration methods with an elastic deformation step using thin-plate splines (TPS). In the absence of definitive ground truth for CaP extent on MRI, classifier accuracy was evaluated against 7 ground truth surrogates obtained by different combinations of the expert and registration segmentations. For 26 multimodal MRI-WMH image pairs, all four registration methods produced a higher area under the receiver operating characteristic curve compared to that obtained from expert annotation. These results suggest that in the presence of additional multimodal image information one can obtain more accurate object annotations than achievable via expert delineation despite vast differences between modalities that hinder image registration.},
	urldate = {2020-03-09},
	booktitle = {Medical {Imaging} 2008: {Computer}-{Aided} {Diagnosis}},
	publisher = {International Society for Optics and Photonics},
	author = {Chappelow, Jonathan and Viswanath, Satish and Monaco, James and Rosen, Mark and Tomaszewski, John and Feldman, Michael and Madabhushi, Anant},
	month = mar,
	year = {2008},
	pages = {69150V},
	file = {Snapshot:/Users/lindronics/Zotero/storage/B7QZKI9Z/12.770703.html:text/html}
}

@article{alom_history_2018,
	title = {The {History} {Began} from {AlexNet}: {A} {Comprehensive} {Survey} on {Deep} {Learning} {Approaches}},
	shorttitle = {The {History} {Began} from {AlexNet}},
	url = {http://arxiv.org/abs/1803.01164},
	abstract = {Deep learning has demonstrated tremendous success in variety of application domains in the past few years. This new field of machine learning has been growing rapidly and applied in most of the application domains with some new modalities of applications, which helps to open new opportunity. There are different methods have been proposed on different category of learning approaches, which includes supervised, semi-supervised and un-supervised learning. The experimental results show state-of-the-art performance of deep learning over traditional machine learning approaches in the field of Image Processing, Computer Vision, Speech Recognition, Machine Translation, Art, Medical imaging, Medical information processing, Robotics and control, Bio-informatics, Natural Language Processing (NLP), Cyber security, and many more. This report presents a brief survey on development of DL approaches, including Deep Neural Network (DNN), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) including Long Short Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). In addition, we have included recent development of proposed advanced variant DL techniques based on the mentioned DL approaches. Furthermore, DL approaches have explored and evaluated in different application domains are also included in this survey. We have also comprised recently developed frameworks, SDKs, and benchmark datasets that are used for implementing and evaluating deep learning approaches. There are some surveys have published on Deep Learning in Neural Networks [1, 38] and a survey on RL [234]. However, those papers have not discussed the individual advanced techniques for training large scale deep learning models and the recently developed method of generative models [1].},
	urldate = {2020-03-14},
	journal = {arXiv:1803.01164 [cs]},
	author = {Alom, Md Zahangir and Taha, Tarek M. and Yakopcic, Christopher and Westberg, Stefan and Sidike, Paheding and Nasrin, Mst Shamima and Van Esesn, Brian C. and Awwal, Abdul A. S. and Asari, Vijayan K.},
	month = sep,
	year = {2018},
	note = {arXiv: 1803.01164},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/lindronics/Zotero/storage/8585T2ZX/Alom et al. - 2018 - The History Began from AlexNet A Comprehensive Su.pdf:application/pdf;arXiv.org Snapshot:/Users/lindronics/Zotero/storage/N9M9FZEH/1803.html:text/html}
}

@article{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	urldate = {2020-03-14},
	journal = {arXiv:1409.1556 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = apr,
	year = {2015},
	note = {arXiv: 1409.1556},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/lindronics/Zotero/storage/Z59D7N2X/Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf;arXiv.org Snapshot:/Users/lindronics/Zotero/storage/YB5ZW826/1409.html:text/html}
}

@inproceedings{szegedy_going_2015,
	title = {Going {Deeper} {With} {Convolutions}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Szegedy_Going_Deeper_With_2015_CVPR_paper.html},
	urldate = {2020-03-14},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	year = {2015},
	pages = {1--9},
	file = {Full Text PDF:/Users/lindronics/Zotero/storage/C8ZWPSLQ/Szegedy et al. - 2015 - Going Deeper With Convolutions.pdf:application/pdf;Snapshot:/Users/lindronics/Zotero/storage/4DKHB6HA/Szegedy_Going_Deeper_With_2015_CVPR_paper.html:text/html}
}

@book{kotikalapudi_keras-vis_2017,
	title = {keras-vis},
	url = {https://github.com/raghakot/keras-vis},
	publisher = {GitHub},
	author = {Kotikalapudi, Raghavendra and {contributors}},
	year = {2017}
}

@misc{iqbal_harisiqbal88plotneuralnet_2018,
	title = {{HarisIqbal88}/{PlotNeuralNet} v1.0.0},
	url = {https://doi.org/10.5281/zenodo.2526396},
	publisher = {Zenodo},
	author = {Iqbal, Haris},
	month = dec,
	year = {2018},
	doi = {10.5281/zenodo.2526396}
}

@book{chollet_keras_2015,
	title = {Keras},
	url = {https://keras.io},
	author = {Chollet, François and {others}},
	year = {2015}
}

@inproceedings{abadi_tensorflow_2016,
	title = {{TensorFlow}: {A} {System} for {Large}-{Scale} {Machine} {Learning}},
	isbn = {978-1-931971-33-1},
	shorttitle = {{TensorFlow}},
	url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi},
	language = {en},
	urldate = {2020-03-16},
	author = {Abadi, Martin and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	year = {2016},
	pages = {265--283},
	file = {Full Text PDF:/Users/lindronics/Zotero/storage/HDJW4UW4/Abadi et al. - 2016 - TensorFlow A System for Large-Scale Machine Learn.pdf:application/pdf;Snapshot:/Users/lindronics/Zotero/storage/AGFEJVH2/abadi.html:text/html}
}

@inproceedings{shi_benchmarking_2016,
	title = {Benchmarking {State}-of-the-{Art} {Deep} {Learning} {Software} {Tools}},
	doi = {10.1109/CCBD.2016.029},
	abstract = {Deep learning has been shown as a successful machine learning method for a variety of tasks, and its popularity results in numerous open-source deep learning software tools coming to public. Training a deep network is usually a very time-consuming process. To address the huge computational challenge in deep learning, many tools exploit hardware features such as multi-core CPUs and many-core GPUs to shorten the training and inference time. However, different tools exhibit different features and running performance when they train different types of deep networks on different hardware platforms, making it difficult for end users to select an appropriate pair of software and hardware. In this paper, we present our attempt to benchmark several state-of-the-art GPU-accelerated deep learning software tools, including Caffe, CNTK, TensorFlow, and Torch. We focus on evaluating the running time performance (i.e., speed) of these tools with three popular types of neural networks on two representative CPU platforms and three representative GPU platforms. Our contribution is two-fold. First, for end users of deep learning software tools, our benchmarking results can serve as a reference to selecting appropriate hardware platforms and software tools. Second, for developers of deep learning software tools, our in-depth analysis points out possible future directions to further optimize the running performance.},
	booktitle = {2016 7th {International} {Conference} on {Cloud} {Computing} and {Big} {Data} ({CCBD})},
	author = {Shi, Shaohuai and Wang, Qiang and Xu, Pengfei and Chu, Xiaowen},
	month = nov,
	year = {2016},
	note = {ISSN: null},
	keywords = {benchmark testing, Benchmark testing, Caffe, CNTK, Convolutional Neural Networks, CPU platform, Deep Learning, deep network training, Feed-forward Neural Networks, feedforward neural nets, GPU, GPU-accelerated deep learning software tools, graphics processing units, Graphics processing units, hardware platforms, Instruction sets, learning (artificial intelligence), Machine learning, machine learning method, microprocessor chips, Neural networks, open-source deep learning software tool benchmarking, performance evaluation, recurrent neural nets, Recurrent Neural Networks, running time performance evaluation, software tools, TensorFlow, Tools, Torch, Training},
	pages = {99--104},
	file = {IEEE Xplore Abstract Record:/Users/lindronics/Zotero/storage/Y2SKET7Q/7979887.html:text/html;IEEE Xplore Full Text PDF:/Users/lindronics/Zotero/storage/U57IBYPS/Shi et al. - 2016 - Benchmarking State-of-the-Art Deep Learning Softwa.pdf:application/pdf}
}