% REMEMBER: You must not plagiarise anything in your report. Be extremely careful.

\documentclass{l4proj}


%
% put any additional packages here
%
\usepackage{bm}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage[export]{adjustbox}

\begin{document}

%==============================================================================
%% METADATA
\title{Deep neural networks for classification of multispectral images}
\author{Niklas Lindorfer}
\date{September 26, 2019}

\maketitle

%==============================================================================
%% ABSTRACT
\begin{abstract}
    Every abstract follows a similar pattern. Motivate; set aims; describe work; explain results.
    \vskip 0.5em
    ``XYZ is bad. This project investigated ABC to determine if it was better. 
    ABC used XXX and YYY to implement ZZZ. This is particularly interesting as XXX and YYY have
    never been used together. It was found that  
    ABC was 20\% better than XYZ, though it caused rabies in half of subjects.''
\end{abstract}

%==============================================================================

% EDUCATION REUSE CONSENT FORM
% If you consent to your project being shown to future students for educational purposes
% then insert your name and the date below to  sign the education use form that appears in the front of the document. 
% You must explicitly give consent if you wish to do so.
% If you sign, your project may be included in the Hall of Fame if it scores particularly highly.
%
% Please note that you are under no obligation to sign 
% this declaration, but doing so would help future students.
%
\def\consentname {Niklas Lindorfer} % your full name
\def\consentdate {26 September 2019} % the date you agree
%
\educationalconsent


%==============================================================================
\tableofcontents

%==============================================================================
%% Notes on formatting
%==============================================================================
% The first page, abstract and table of contents are numbered using Roman numerals and are not
% included in the page count. 
%
% From now on pages are numbered
% using Arabic numerals. Therefore, immediately after the first call to \chapter we need the call
% \pagenumbering{arabic} and this should be called once only in the document. 
%
% The first Chapter should then be on page 1. You are allowed 40 pages for a 40 credit project and 20 pages for a 
% 20 credit report. This includes everything numbered in Arabic numerals (excluding front matter) up
% to but excluding the appendices and bibliography.
%
% You must not alter text size (it is currently 10pt) or alter margins or spacing.
%
%
%==================================================================================================================================
%
% IMPORTANT
% The chapter headings here are **suggestions**. You don't have to follow this model if
% it doesn't fit your project. Every project should have an introduction and conclusion,
% however. 
%
%==================================================================================================================================
\chapter{Introduction}

% reset page numbering. Don't remove this!
\pagenumbering{arabic} 


Why should the reader care about what are you doing and what are you actually doing?



%==================================================================================================================================
\chapter{Background}
What did other people do, and how is it relevant to what you want to do?

\section{Imaging}

\subsection{Electromagnetic radiation}

Visible light and infrared are forms of electromagnetic radiation. Visible light (VIS) has wavelengths roughly between $400$ and $700 nm$. The hue of visible light is determined by its wavelength. Colours as perceived by human vision are the result of a spectral distribution of wavelengths of the light than enters the eye. 

Infrared light (IR), with wavelengths between $700 nm$ and $1 mm$, is generally invisible to humans. It is commonly subdivided into near infrared (NIR), short-wavelength infrared (SWIR), medium-wavelength infrared (MWIR), long-wavelength infrared (LWIR), and far infrared (FIR) \citep[p. 28]{byrnes_unexploded_2008}, as can be seen in Figure \ref{fig:em_spectrum}.

\begin{figure}[ht]
  \centering
  \begin{subfigure}[h!]{0.9\textwidth}
    \includegraphics[width=\textwidth, trim={1.5cm 4cm 2cm 4cm}, clip=true]{images/EM_spectrum.pdf}
  \end{subfigure}
  \caption{The electromagnetic spectrum with detailled view of visible light and infrared. IR subdivisions modelled after \citet[p. 28]{byrnes_unexploded_2008}.}
  \label{fig:em_spectrum}
\end{figure}

\subsection{Thermal infrared radiation}

Black-body radiation is defined as thermal electromagnetic radiation emitted by an idealized opaque, non-reflective body \citep{young_sears_2012}. It has a continuous frequency spectrum that is dependent on the temperature of the body \citep{kogure_thermodynamic_2007}. The spectrum has a characteristic peak, which is antiproportional to the temperature $T [K]$ according to Wien's displacement law:

\begin{equation}
  \lambda_{peak} = \frac{2.898 \times 10^{-3}}{T} [m],
\end{equation}

Bodies at a room temperature of $300 K$ emit electromagnetic radiation with a peak at $9.7 \mu m$ \citep{jarc_graz_2007}, falling into the LWIR range. To account for different non-ideal surfaces, the material-specific emissivity constant $\epsilon \in [0..1]$ is introduced to describe the share of radiation that is emitted by a body compared to an ideal black body. The total energy radiated by a body per unit surface and time is expressed by the Stefan-Boltzmann law:

\begin{equation}
  W = \epsilon \sigma T^4,
\end{equation}

where $\sigma$ is a proportionality constant. Thus, if the emissivity of the body is known, this law can be applied to determine its temperature. This is the theoretical foundation for thermal imaging. In practice, the material and corresponding emissivity are often undetermined. Therefore, the received infrared radiation contains an unknown share of reflected radiation, adding noise to the measurement. This can be somewhat mitigated by estimating the value of $\epsilon$.

\subsection{Spectral imaging}

Spectral images combine spatial and spectral information in a 3-D data structure where the dimensions correspond to height, width and waveband. 
A distinction is made between hyperspectral and multispectral imaging techniques. 

If an image is acquired at many (tens or hundreds) regularly sampled wavebands, one speaks of a hyperspectral image, sometimes referred to as a hypercube. Hyperspectral images have many powerful applications, including agriculture and food quality \citep{dale_hyperspectral_2013} and medical applications \citep{lu_medical_2014}. However, capturing hyperspectral images requires advanced sensing equipment. Furthermore, hypercubes are difficult to process due to the high amount of information and therefore often require the application of computationally expensive preprocessing techniques, such as dimensionality-reduction \citep{qin_hyperspectral_2013}.

Multispectral images comprise of a few discrete wavebands. As opposed to hyperspectral images, it is usually not possible to extract a full continuous spectrum of an individual pixel from multispectral images \citep{abdul_multi-disnet_2019}.


%----------------------------------------------------------------------------------------------------------------------------------

\section{Machine learning}

\subsection{Supervised and unsupervised algorithms}

Machine learning techniques can be categorised into supervised, semi-supervised, unsupervised, and reinforcement learning \citep{burkov_hundred-page_2019}.

In supervised learning, a dataset consisting of a list of labelled features $\{(\vec{x}_i, y_i)\}^N_{i=1}$ is provided. $\vec{x_i}$ is called a feature vector. The supervised learning algorithm then attempts to fit a model $f$ on the dataset such that $\forall{\vec{x_i}}. (f(\vec{x_i}) = y_i)$. The model can then be used to predict the unknown label of a test data point. In practice, a perfect fit is not possible due to noise in the dataset. Therefore, supervised algorithms are evaluated according to a loss function measuring the difference between $f(\vec{x_i})$ and $y_i$. When fitting the model, the algorithm attempts to minimise the loss over the training data. Common choices for loss functions are mean squared error (MSE) for regression and categorical cross-entropy for classification problems.

Unsupervised learning involves a dataset of unlabeled data $\{\vec{x}_i\}^N_{i=1}$. An unsupervised learning algorithm attempts to transform each feature vector $\vec{x}_i$ into another vector to solve a practical problem, such as dimensionality reduction or outlier detection \citep{burkov_hundred-page_2019}. 
This project focuses on supervised learning.

\subsection{Regression and classification}

In supervised learning, one speaks of classification or regression, depending on whether the data labels $y_i$ are discrete classes or continous, real values.

\subsection{Deep learning}






%==================================================================================================================================

\chapter{Analysis/Requirements}
What is the problem that you want to solve, and how did you arrive at it?

\section{Data generation}

Collecting a multispectral dataset is significantly more challenging than a dataset consisting solely of visible-light images. The majority of image classification tasks use traditional imaging only (XXX citation needed). Consequently, the amount of research and resources available to use is much more limited.

\subsection{Transfer learning}

A large amount of existing datasets is available for traditional image classification tasks. The ImageNet dataset \citep{deng_imagenet_2009}, which is used as a benchmark for many image classification models, contains over a million images ready to be used. Many successful deep learning models, such as AlexNet \citep{krizhevsky_imagenet_2012} and ResNet \citep{he_deep_2016} have been applied to datasets such as ImageNet.

When creating a new image classifier, it is then possible to incorporate pretrained versions of these networks into a new classifier and perform transfer learning on a new dataset. This strategy makes it feasible to train complex models on smaller amounts of data than usual, as many meaningful weights have already been learned by the pretrained model (XXX citation needed).

For multispectral models, this approach becomes more difficult, as much fewer datasets are available to be used off-the-shelf. Datasets for visible light and thermal data, such as the KAIST Pedestrian Detection Benchmark \citep{hwang_multispectral_2015} are available. However, They usually lack the variety and size of traditional computer vision datasets and are therefore less suitable for general tasks.

Pre-training a multispectral classifier on an VIS-only dataset is not directly possible due to the incompatible input tensor shapes ($h \times w \times 3$ compared to $h \times w \times 4$) and the resulting difference in trainable weights.

However, there are strategies to mitigate this issue. It is possible to divide the model into 2 separate branches of layers that process the VIS and LWIR images individually, and perform late feature fusion by concatenating the respective feature maps and adding at least one fully connected layer after the concatenation. This approach would make it possible to load pretrained weights into the VIS-branch of the model without affecting the LWIR branch. During training, the LWIR-branch of the model would get trained from scratch, whereas transfer learning is simultaneously performed on the VIS-branch.

Another possible approach to enable the usage of existing off-the-shelf datasets is to perform generative image augmentation. In Section \ref{autoencoder_implementation} we explore the application of a primitive generative model for predicting the corresponding LWIR image of a known type of object from a given RGB image. Given sufficient quality of predicted images, it would then be possible to turn RGB-only images into multispectral training data.


\subsection{Sensory equipment}

Even though thermal cameras have become significantly more affordable in recent years, their resolution remains much lower than that of conventional visible-light cameras. The FLIR One Pro, which was used for this project, is composed of two sensors:

\begin{itemize}
  \item A conventional camera for visible-light images with a resolution of $1440 \times 1080$ px.
  \item A thermal sensor with a resolution of $160 \times 120$ px and a spectral range of $8 - 14 \mu m$.
\end{itemize}

The sensor is therefore able to perceive and record electromagnetic waves in the visible-light (VIS) and long wave infrared (LWIR) bands. Due to technical limitations, several challenges had to be overcome when working with data collected by the sensor.

Firstly, the two cameras are not spatially aligned. Furthermore, the "zoom level" or focal length of the sensors does not appear to match. A misalignment of the channels is likely to have an impact on the performance of a convolutional network (XXX citation needed). Therefore, the issue of image registration of the two camera outputs arises. The strategy we introduced to perform robust registration is outlined in Section \ref{image_registration}.

Secondly, the vastly different resolutions of the sensors could cause problems, as convolutional filters might struggle detecting edges and other features (XXX).

%----------------------------------------------------------------------------------------------------------------------------------

\section{Mobile application}

This project aims to deploy the final deep learning model to a mobile edge device. This introduces many challenges related to mobile computing. Contrary to most computers, computational power on mobile devices is highly limited. Furthermore, to provide robust and reliable real-time classification, low latency between image capture and classification will be necessary. Two major strategies can be used to apply image classification on a mobile device:

\begin{enumerate}
  \item Deploy the model to a web service, upload captured frames from edge device, and retrieve the prediction.
  \item Deploy the model to the edge device and perform all computations "offline".
\end{enumerate}

To minimise latency, the second approach was selected. It will therefore be possible to use the application even in situations with poor connectivity. However, the necessity to perform all computational tasks on the edge-device imposes many problems. The Tensorflow SDK for Java is very limited compared to the Python API. Furthermore, various image pre-processing tasks have to be performed without the standard Python data science stack that was used for training and experimentation. Many image processing libraries do not support multispectral data, further complicating image processing.

The mobile application will furthermore provide the ability to capture datasets in a convenient way, without the need to seperately extract raw RGB and LWIR images after capture.


%==================================================================================================================================
\chapter{Design}
How is this problem to be approached, without reference to specific implementation details? 


\section{Dataset}

\subsection{Classes and samples}

Initially, a dataset consisting of 13 classes and 1042 samples was captured, using the camera functionality of the mobile application. The app captures images in quick temporal succession, effectively like a video. Thus, many images are relatively similiar, as they were captured within only a few milliseconds difference. To distinguish between different "bursts" of images, images were grouped into distinct subsets, each subset representing a batch of similar images.

Due to the difficulty of obtaining well-framed and unobstructed images, the data contains some noise. Some images contain more than one animal. Furthermore, some animals were housed in closed-off shelters and were only visible behind a metal grid. Therefore, each subset was labeled as either \textit{single}, \textit{multi}, or \textit{obstructed}, to allow greater flexibility when selecting data for training and validation. Moreover, no distinction between genders was made. This primarily impacts the peacock class, as male and female peacocks look fairly different.

\begin{table}[ht]
  \centering
  \begin{tabular}{@{}ll@{}ll@{}}
    \toprule
    \textbf{Class}  & \textbf{Dataset 1 } & \textbf{Dataset 2} \\ \midrule
    Cat             & 205              & 0 \\
    Pig             & 30               & 0 \\
    Pony            & 112              & 127 \\
    Sheep           & 36               & 0 \\
    Alpaca          & 85               & 391 \\
    Ferret          & 33               & 0 \\
    Peacock         & 157              & 79 \\
    Hamburg chicken & 29               & 58 \\
    Evil chicken    & 17               & 129 \\
    Ugly duck       & 24               & 141 \\
    Chicken         & 138              & 151 \\
    Rabbit          & 32               & 16 \\
    Goose           & 144              & 0 \\ \bottomrule
  \end{tabular}
  \caption{Classes and sample size of raw animals datasets.}
  \label{table:raw_dataset}
\end{table}

Initially, the split between training and validation data was performed by randomly drawing samples from the dataset in a stratified way. As demonstrated in (XXX link), nearly perfect validation accuracy was achieved on this data. This most likely being a result of the aforementioned similarity of many images due to them being captured at similar times, we decided that a second, independent dataset was necessary to achieve representative evaluation results. The classes and samples of both datasets are shown in Table \ref{table:raw_dataset}.

Finally, the subsets from both datasets were rearranged into a training dataset and a testing dataset. An attempt was made to obtain a balanced mixture of \textit{single}, \textit{multi}, and \textit{obstructed} subsets in both datasets. Nonetheless, it can be assumed that achieving a perfect, representative split is difficult due to the noise in the dataset and the limited variety of contexts. The train-test-split is shown in Table \ref{table:train_test_dataset}.


\begin{table}[ht]
  \centering
  \begin{tabular}{@{}ll@{}ll@{}}
    \toprule
    \textbf{Class}  & \textbf{Train } & \textbf{Test} \\ \midrule
    Cat             & 149              & 56 \\
    Pony            & 211              & 28 \\
    Alpaca          & 406               & 70 \\
    Peacock         & 157              & 79 \\
    Hamburg chicken & 58               & 29 \\
    Evil chicken    & 129               & 17 \\
    Ugly duck       & 141               & 24 \\
    Chicken         & 211              & 78 \\
    Goose           & 125              & 19 \\ \bottomrule
  \end{tabular}
  \caption{Classes and sample size train and test datasets.}
  \label{table:train_test_dataset}
\end{table}

The classes with low support (Pig, Sheep, Ferret, Rabbit) were dropped from the datasets, as it was deemed unlikely that a model would be able to accurately be fit on such small numbers of samples.

Figure \ref{fig:dataset_classes} shows example RGB and LWIR images from the training dataset.


\begin{figure}[ht]
  \centering
  \begin{subfigure}[h!]{0.18\textwidth}
    \includegraphics[width=\textwidth]{images/dataset/cat/rgb.png}
    \includegraphics[width=\textwidth]{images/dataset/cat/lwir.png}
    \caption{Cat}
  \end{subfigure}
  \begin{subfigure}[h!]{0.18\textwidth}
    \includegraphics[width=\textwidth]{images/dataset/pony/rgb.png}
    \includegraphics[width=\textwidth]{images/dataset/pony/lwir.png}
    \caption{Pony}
  \end{subfigure}
  \begin{subfigure}[h!]{0.18\textwidth}
    \includegraphics[width=\textwidth]{images/dataset/alpaca/rgb.png}
    \includegraphics[width=\textwidth]{images/dataset/alpaca/lwir.png}
    \caption{Alpaca}
  \end{subfigure}
  \begin{subfigure}[h!]{0.18\textwidth}
    \includegraphics[width=\textwidth]{images/dataset/peacock/rgb.png}
    \includegraphics[width=\textwidth]{images/dataset/peacock/lwir.png}
    \caption{Peacock}
  \end{subfigure}
  \begin{subfigure}[h!]{0.18\textwidth}
    \includegraphics[width=\textwidth]{images/dataset/pretty_chicken/rgb.png}
    \includegraphics[width=\textwidth]{images/dataset/pretty_chicken/lwir.png}
    \caption{Hamburg chicken}
  \end{subfigure}
  \begin{subfigure}[h!]{0.18\textwidth}
    \includegraphics[width=\textwidth]{images/dataset/evil_chicken/rgb.png}
    \includegraphics[width=\textwidth]{images/dataset/evil_chicken/lwir.png}
    \caption{Evil chicken}
  \end{subfigure}
  \begin{subfigure}[h!]{0.18\textwidth}
    \includegraphics[width=\textwidth]{images/dataset/ugly_duck/rgb.png}
    \includegraphics[width=\textwidth]{images/dataset/ugly_duck/lwir.png}
    \caption{Ugly duck}
  \end{subfigure}
  \begin{subfigure}[h!]{0.18\textwidth}
    \includegraphics[width=\textwidth]{images/dataset/chicken/rgb.png}
    \includegraphics[width=\textwidth]{images/dataset/chicken/lwir.png}
    \caption{Chicken}
  \end{subfigure}
  \begin{subfigure}[h!]{0.18\textwidth}
    \includegraphics[width=\textwidth]{images/dataset/goose/rgb.png}
    \includegraphics[width=\textwidth]{images/dataset/goose/lwir.png}
    \caption{Goose}
  \end{subfigure}
  \caption{Examples for each class in the final dataset.}
  \label{fig:dataset_classes}
\end{figure}

\subsection{Data augmentation}

Due to the limited amount of resources available to the project, generating a dataset with sufficient scope and size is challenging. Since the performance of machine learning models is usually directly tied to the size and quality of the available dataset, an insufficient dataset might make it impossible to train an accurate and robust classifier \citep{fawzi_adaptive_2016}. Therefore, data augmentation will have to be employed to artificially increase the amount of distinct data samples.

\citet{mikolajczyk_data_2018} make the distinction between white-box and black-box data augmentation methods. While the latter use a form of deep learning, namely Generative Adversarial Networks (GAN), to synthesise new training samples, white-box methods involve more traditional techniques, such as affine transformations. 

After evaluating various methods of data augmentation on a subset of the ImageNet dataset, \citet{perez_effectiveness_2017} conclude that traditional affine transformations alone can be very effective, although GAN-based methods are promising and might yield better results.

Due to the relatively easy implementation, this project therefore primarily uses white-box data augmentation. However, the design and implementation of a deep-learning model generating LWIR from visible light is discussed in Section \ref{autoencoder_implementation}. This autoencoder could be used to generate new LWIR data samples from visible light images retrieved from other sources, such as existing computer vision datasets.

Figure \ref{fig:augmentation_affine} shows different affine transformations that were used to augment the captured dataset.

\begin{figure}[ht]
  \centering
  \begin{subfigure}[h!]{0.24\textwidth}
    \includegraphics[width=\textwidth]{images/augmentation/original.png}
    \caption{Original image}
  \end{subfigure}
  \begin{subfigure}[h!]{0.24\textwidth}
    \includegraphics[width=\textwidth]{images/augmentation/zoomed.png}
    \caption{Zoomed in}
  \end{subfigure}
  \begin{subfigure}[h!]{0.24\textwidth}
    \includegraphics[width=\textwidth]{images/augmentation/rotated.png}
    \caption{Rotated $20^{\circ}$}
  \end{subfigure}
  \begin{subfigure}[h!]{0.24\textwidth}
    \includegraphics[width=\textwidth]{images/augmentation/flipped.png}
    \caption{Horizontally flipped}
  \end{subfigure}
  \caption{Affine data augmentation strategies applied to a sample LWIR image.}
  \label{fig:augmentation_affine}
\end{figure}

Another possible way of augmenting the data before classification is artificially drawing obstacles over the image samples. 


%----------------------------------------------------------------------------------------------------------------------------------

\section{Mobile application}



%==================================================================================================================================
\chapter{Implementation}
What did you do to implement this idea, and what technical achievements did you make?

\section{Image registration: aligning the RGB and LWIR images}
\label{image_registration}

The images captured by the visible light camera and thermal sensor of the FLIR One Pro are not properly aligned by default. This can have adverse consequences to the classification performance of a machine learning model (XXX citation needed). Thus, an algorithm for properly aligning the images captured by the conventional camera and LWIR camera had to be developed. The process of transforming data from different sensors into one coordinate system is commonly referred to as image registration.

On superficial inspection, it is apparent that the LWIR sensor appears to have a larger focal length than the RGB camera, and therefore has a narrower field of view. Hence, an attempt was made to manually align the LWIR and RGB images by cropping and rescaling the RGB image, effectively "zooming in". This approach yielded moderate results, as can be seen in (XXX).

To obtain a more flexible and generalisable representation, we made use of affine transformations. An affine transformation or affine map can accurately represent a composition of a translation and a linear map (e.g. scaling, rotation and shearing). A transformation $f(\vec{x})$ can be expressed as follows:

\begin{equation}
  f(\vec{x}) = A \vec{x} + \vec{b},
\end{equation}

where $\vec{x}$ represents the coordinates of a point to be translated, $A$ is a $2 \times 2$ linear map, and $\vec{b}$ is the translation vector.

Subsequently, the coordinates of about a dozen reference points were manually determined on the RGB and LWIR versions of some test images. These coordinates were used as inputs and outputs to train a linear regression model representing the affine transformation required align the RGB image with the LWIR image. 

% The effect of applying the resulting transformation to a set of coordinates are shown in Figure \ref{fig:transformation_arrows}.

% \begin{figure}[ht]
%   \centering
%   \begin{subfigure}[h!]{0.45\textwidth}
%     \includegraphics[width=\textwidth]{images/registration/transformation_arrows.png}
%     \caption{Synthetic image, black on white.}
%     \label{fig:transformation_arrows}
%   \end{subfigure}
%   \caption{}
% \end{figure}

\begin{figure}[ht]
  \centering
  \begin{subfigure}[h!]{0.3\textwidth}
    \includegraphics[width=\textwidth, trim={3.5cm 9cm 3.5cm 4.5cm}, clip, frame]{images/registration/unregistered.png}
    \caption{Before registration. The LWIR and VIS images are visibly misaligned.}
  \end{subfigure}
  \begin{subfigure}[h!]{0.3\textwidth}
    \includegraphics[width=\textwidth, trim={3.5cm 9cm 3.5cm 4.5cm}, clip, frame]{images/registration/registered_zoom.png}
    \caption{After zooming in. The images are almost aligned. A slight offset remains.}
  \end{subfigure}
  \begin{subfigure}[h!]{0.3\textwidth}
    \includegraphics[width=\textwidth, trim={3.5cm 9cm 3.5cm 4.5cm}, clip, frame]{images/registration/registered_affine.png}
    \caption{After registration. The thermal signatures of the candles align with the VIS location.}
  \end{subfigure}
  \caption{Superimposed LWIR and VIS images before and after registration. Gradients inverted for better visibility.}
  \label{fig:linear_trans_before_after}
\end{figure}

\subsection{Intensity-based registration metrics}

As can be seen in Figure \ref{fig:linear_trans_before_after}, the the candles on the images now line up. To quantify the accuracy of the alignment, attempts were made to use various metrics for image similarity. This task is difficult, as the properties of objects are vastly different under visible light and far infrared. Therefore, a simple intensity-based metric is unlikely to be robust in this scenario, as local intensities will vary across different channels even if the registration is perfect \citep{myronenko_intensity-based_2010}.

To mitigate this problem \citet{chen_normalized_2018} introduce the Normalized Total Gradient (NTG) metric for multispectral imaging systems, which is defined as follows:

\begin{equation}
  NTG(f, f_R) = \frac{\sum_l |\nabla_l \{f - f_R\}|}{\sum_l | \nabla_l f | + \sum_l | \nabla_l f_R|},
\end{equation}

where $f$ and $f_R$ are the channel images to be compared, $\nabla_l$ represents the gradient computation along the direction $l \in \{x, y\}$, and $| \cdot |$ denotes the L1-norm.

In other words, to obtain NTG one computes the sum of the $x$- and $y$-gradients of the difference of the two channels, and normalises the result by dividing over the sum of gradients of the individual channels.

The metric is based on the assumption that the gradient of the channel difference image becomes sparser as the alignment improves. Figure \ref{fig:gradient_distribution} shows that this is hardly the case for images captured by the LWIR camera, as opposed to the baseline of comparing the red and green channels from the RGB camera.

This might be due to the fact that the resolution of output LWIR files is $640 \times 420$ px, although the actual resolution of the sensor is only $160 \times 120$. Thus, when downsampling the RGB image to $640 \times 420$ px, edges are still much sharper than for the corresponding LWIR image. Downsampling both images to the native resolution of the LWIR camera did not yield better results.

Moreover, an attempt at smoothing both images before calculating the NTG was made by applying the Laplacian of Gaussian (LoG) with $\sigma=2$ to both images. As can be seen in Table \ref{table:registration_ntg}, this approach was not successful either. 

Considering these results, it can be concluded that NTG is not a suitable metric for this particular problem. As the baseline seems to work, it is possible that the properties of the LWIR and RGB channels are too different for an intensity-based approach.

\begin{figure}[ht]
  \centering
  \begin{subfigure}[h!]{0.45\textwidth}
    \includegraphics[width=\textwidth]{images/registration/gradient_distribution.png}
    \caption{LAB intensity of visible light image vs thermal intensity at $640 \times 420$ px}
    \label{fig:gradient_distribution}
  \end{subfigure}
  \begin{subfigure}[h!]{0.45\textwidth}
    \includegraphics[width=\textwidth]{images/registration/gradient_distribution_red_green.png}
    \caption{Red channel intensity vs green channel intensity at $640 \times 420$ px}
    \label{fig:gradient_distribution_red_green}
  \end{subfigure}
  \caption{Distribution of gradients of the channel difference image before and after alignment using affine transformation.}
\end{figure}


\begin{table}[ht]
  \centering
  \begin{tabular}{@{}lll@{}}
  \toprule
  \textbf{Test configuration}                             & \textbf{Misaligned} & \textbf{Aligned} \\ \midrule
  LAB intensity of RGB vs LWIR (640x480)     & 0.9984              & 0.9987           \\
  LAB intensity of RGB vs LWIR (160x120)    & 0.9974              & 0.9986           \\
  LAB intensity of RGB vs LWIR (after LoG with $\sigma=2$) & 0.9970              & 0.9973           \\
  (Baseline) Red vs green channels of RGB (160x120)                  & 0.6245              & 0.0303           \\ \bottomrule
  \end{tabular}
  \caption{Normalized Total Gradient before and after channel alignment}
  \label{table:registration_ntg}
\end{table}


\subsection{Texture-based registration}

As it has been demonstrated that intensity-based metrics are inadequate in this scenario, an approach based on image texture features, as proposed by \citet{jarc_graz_2007} was tested. 

The method uses pairs of 1-D filter masks as introduced by \citet{laws_rapid_1980} to detect level, edge, spot and ripple features. These filters can be applied horizontally and vertically. In our implementation, all possible filter pairs were evaluated and the final scores were averaged.

After convoluting both the LWIR and RGB images with the filters, the results were turned into texture energy images by performing a convolutional sum-operation of absolute values using a $15 \times 15$ kernel and applying a threshold of $\pm 3 \sigma$. 

Finally, the mutual information (MI) of the resulting LWIR and RGB images was calculated as follows:

\begin{equation}
  MI(A,B) = H(A) + H(B) - H(A,B),
\end{equation}

where $H(X) = - \sum_{x \in X} p(x) \cdot log_2 p(x)$ is the entropy of $X$ and $H(X,Y)$ represents the joint entropy of $X$ and $Y$.

%----------------------------------------------------------------------------------------------------------------------------------


\section{Predicting LWIR images using an autoencoder}
\label{autoencoder_implementation}

\subsection{Traditional autoencoder}

A deep convolutional autoencoder was created to predict the LWIR signature of objects from the corresponding visible light image. The architecture of the network is shown in Figure \ref{fig:autoencoder_architecture}. The network consists of an encoder and a decoder that each contain four blocks of layers.

Each encoder block has a convolutional layer, followed by a leaky ReLU activation function and a subsequent max pooling layer for downsampling. The $320 \times 240 \times 1$ input images are thus processed and downsampled to a $20 \times 15 \times 32$ tensor containing a compact, latent representation of the features in the original image.

The output of the decoder is then fed forward into four blocks of deconvolutional layers followed by leaky ReLU activation to attempt a reconstruction the LWIR image. Each deconvolutional layer tries to upsample the input by predicting appropriate pixel values.

For this task, a subset of the animals dataset with 336 images of Shetland ponies was chosen. Mean squared error was selected as the loss function. The network was trained for 100 epochs with a batch size of 16. Training time averaged around 18 minutes on an NVIDIA Tesla P-100 GPU.

After successful training of the network, two images of ponies were retrieved from Wikimedia Commons. They were then fed into the network in an attempt to generate the corresponding LWIR images. The output of the autoencoder can be seen in Figure \ref{fig:autoencoder_pony}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\textwidth]{images/autoencoder/autoencoder}
  \caption{Neural network architecture of the autoencoder.}
  \label{fig:autoencoder_architecture}
\end{figure}

\subsection{U-Net}

U-Net was first introduced by \citet{ronneberger_u-net_2015} to improve performance of biomedical image segmentation systems. Similarly to a conventional autoencoder, it features a contracting path that downsamples or encodes the input, and a symmetric expanding path that upsamples or decodes the input. Additionally, for each deconvolutional layer, the output of the corresponding opposite convolutional layer is concatenated to the layer's input map. These connections can be seen in Figure \ref{fig:unet_architecture}. 

Similar to the traditional autoencoder, the U-Net was trained for 100 epochs on the the same dataset. Figure \ref{fig:autoencoder_pony} shows the output of the U-Net and compares it to the autoencoder output.



\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\textwidth]{images/unet/unet}
  \caption{Neural network architecture of the U-Net.}
  \label{fig:unet_architecture}
\end{figure}

\begin{figure}[ht]
  \centering
  \begin{subfigure}[h!]{0.25\textwidth}
    \includegraphics[width=\textwidth]{images/unet/pony_grayscale.png}
    \includegraphics[width=\textwidth]{images/unet/pony_2_grayscale.png}
    \caption{Original images (cropped)}
  \end{subfigure}
  \begin{subfigure}[h!]{0.25\textwidth}
    \includegraphics[width=\textwidth]{images/autoencoder/pony_predicted.png}
    \includegraphics[width=\textwidth]{images/autoencoder/pony_2_predicted.png}
    \caption{Autoencoder prediction}
  \end{subfigure}
  \begin{subfigure}[h!]{0.25\textwidth}
    \includegraphics[width=\textwidth]{images/unet/pony_predicted.png}
    \includegraphics[width=\textwidth]{images/unet/pony_2_predicted.png}
    \caption{U-Net prediction}
  \end{subfigure}
  \caption{Predicted LWIR signature of images of shetland ponys. Retrieved from \url{https://commons.wikimedia.org/wiki/File:Sandwick_Shetland_Pony.jpg} and \url{https://commons.wikimedia.org/wiki/File:Shetland_ponies_at_Sandwick.jpg}}
  \label{fig:autoencoder_pony}
\end{figure}




% \begin{figure}[ht]
%   \centering
%   \begin{subfigure}[h!]{0.3\textwidth}
%     \includegraphics[width=\textwidth]{images/autoencoder/mirror/grayscale.png}
%     \caption{Grayscale of visible light}
%     \label{fig:gradient_distribution}
%   \end{subfigure}
%   \begin{subfigure}[h!]{0.3\textwidth}
%     \includegraphics[width=\textwidth]{images/autoencoder/mirror/pred_fir.png}
%     \caption{Predicted LWIR}
%     \label{fig:gradient_distribution_red_green}
%   \end{subfigure}
%   \begin{subfigure}[h!]{0.3\textwidth}
%     \includegraphics[width=\textwidth]{images/autoencoder/mirror/actual_fir.png}
%     \caption{Actual LWIR}
%     \label{fig:gradient_distribution_red_green}
%   \end{subfigure}
%   \caption{Autoencoder prediction of LWIR image from grayscale.}
% \end{figure}


\section{Mobile application}

The mobile application was implemented using Java and Android Studio. 



%==================================================================================================================================

\chapter{Evaluation} 
How good is your solution? How well did you solve the general problem, and what evidence do you have to support that?

\section{Methods and hyperparameters}

\subsection{Data augmentation}

\subsection{Generation of artificial data}

\subsection{Data sampling}

\subsection{VIS only compared to VIS+LWIR}

%----------------------------------------------------------------------------------------------------------------------------------

\section{Different network designs}

\subsection{Custom CNN}

\subsection{AlexNet}

\subsection{ResNet}

%----------------------------------------------------------------------------------------------------------------------------------

\section{Computational performance on mobile device}


\begin{figure}[ht]
  \centering
  \begin{subfigure}[h!]{0.6\textwidth}
    \includegraphics[width=\textwidth]{images/evaluation/confusion/vis_fir_confusion.png}
    \caption{Confusion matrix}
    \label{fig:confusion}
  \end{subfigure}
  \caption{ASDF}
\end{figure}


%==================================================================================================================================
\chapter{Conclusion}    
Summarise the whole project for a lazy reader who didn't read the rest (e.g. a prize-awarding committee).

%==================================================================================================================================
%
% 
%==================================================================================================================================
%  APPENDICES  

\begin{appendices}

\chapter{Appendices}

Typical inclusions in the appendices are:

\begin{itemize}
\item
  Copies of ethics approvals (required if obtained)
\item
  Copies of questionnaires etc. used to gather data from subjects.
\item
  Extensive tables or figures that are too bulky to fit in the main body of
  the report, particularly ones that are repetitive and summarised in the body.

\item Outline of the source code (e.g. directory structure), or other architecture documentation like class diagrams.

\item User manuals, and any guides to starting/running the software.

\end{itemize}

\textbf{Don't include your source code in the appendices}. It will be
submitted separately.

\end{appendices}

%==================================================================================================================================
%   BIBLIOGRAPHY   

% The bibliography style is abbrvnat
% The bibliography always appears last, after the appendices.

\bibliographystyle{abbrvnat}

\bibliography{l4proj}

\end{document}
